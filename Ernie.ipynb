{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ernie / Ernie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/blob/ernie/Ernie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRInyJ5BklIx",
        "colab_type": "code",
        "outputId": "ff35a271-a05e-41b7-e9db-93452f57733d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7frLgMWBG-z",
        "colab_type": "code",
        "outputId": "c0208350-7369-4016-f9af-d19d5c5437f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "!pip install paddlepaddle-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.5)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.5)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.4)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: paddlepaddle-gpu in /usr/local/lib/python3.6/dist-packages (1.6.3.post107)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (1.0.2)\n",
            "Requirement already satisfied: nltk; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.2.5)\n",
            "Requirement already satisfied: scipy; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (1.4.1)\n",
            "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (1.12.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (0.10.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.13)\n",
            "Requirement already satisfied: objgraph in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.12; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (1.17.5)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.10.0)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (3.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (0.7.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (2.21.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (4.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from paddlepaddle-gpu) (4.1.2.30)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle-gpu) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle-gpu) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle-gpu) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle-gpu) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->paddlepaddle-gpu) (45.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->paddlepaddle-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->paddlepaddle-gpu) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->paddlepaddle-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->paddlepaddle-gpu) (2019.11.28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yk1KF2QClpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tqdm import tqdm\n",
        "from google.colab import auth, drive\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-2A9THPfViR",
        "colab_type": "code",
        "outputId": "1315e2f8-9cf4-4d6b-b81a-fa1c2ad0e278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "print(training_dataset.head())\n",
        "test_tweets_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/testset-levela.tsv'\n",
        "test_tweets = pd.read_csv(test_tweets_url, delimiter='\\t')\n",
        "print(test_tweets.head())\n",
        "test_labels_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/labels-levela.csv'\n",
        "test_labels = pd.read_csv(test_labels_url, delimiter=',', header=0, names=[\"id\", \"label\"])\n",
        "print(test_labels.head())\n",
        "test_dataset = test_tweets.set_index(\"id\").join(test_labels.set_index(\"id\"))\n",
        "test_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "      id                                              tweet\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
            "3  13876  #Watching #Boomer getting the news that she is...\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...\n",
            "      id label\n",
            "0  15923   OFF\n",
            "1  27014   NOT\n",
            "2  30530   NOT\n",
            "3  13876   NOT\n",
            "4  60133   OFF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27014</th>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30530</th>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet label\n",
              "id                                                            \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF\n",
              "27014  #ConstitutionDay is revered by Conservatives, ...   NOT\n",
              "30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT\n",
              "13876  #Watching #Boomer getting the news that she is...   NOT\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right...   OFF"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrWT8auffaHx",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoLqOPNzfXsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 13\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = test_dataset['tweet'][test_dataset['label'].notnull()]\n",
        "test_labels_A = (test_dataset['label'][test_dataset['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "training_x = np.asarray(training_examples_A)\n",
        "validation_x = np.asarray(validation_examples_A)\n",
        "test_x = np.asarray(test_examples_A)\n",
        "training_y = np.asarray(training_labels_A)\n",
        "validation_y = np.asarray(validation_labels_A)\n",
        "test_y = np.asarray(test_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIx5-CLpffCo",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slqijwp6fqEy",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0TyndvDfds1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffÃ¨ -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF7yd2-ofsPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha_QGMCegBNp",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcKKJJvwfuFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        if flag == True:\n",
        "            suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            if len(suggestions) > 0:\n",
        "                edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I1NZ62HgDph",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YAsJrM-gFvd",
        "colab_type": "code",
        "outputId": "5417a12c-728c-49f9-a550-8253e9c85b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# normalization -> papers, complicated solutions, replace abbreviations with full names (e.g. MAGA)\n",
        "# check removing less stop words (some may have some significance)\n",
        "\n",
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@USER', '@USER', '@USER', 'know', 'belong', 'violent', 'liberal', 'Antifa', 'party', 'w/', 'KKK', 'HOODS', 'beat', 'total', 'stranger', '&', 'amp', 'fund', 'party', 'violent', '&', 'amp', 'corrupt', 'party', 'try', 'pour', 'tear', '&', 'amp', 'believe', 'heart', 'hard', 'sell', 'include', 'voter'], ['@user', 'lie', 'corrupt', 'traitor', 'want', 'hear', 'anymore', 'lie', 'deepstatecorruption', 'url'], ['@USER', '@USER', 'typical', 'liberal', 'principle', 'come', 'money'], ['@user', 'know', 'add', 'million', '@USER', 'donation', 'receive', 'access'], ['@user', 'idiot', 'fringe', 'right', 'hate', 'conservative'], ['@USER', '@USER', '@USER', 'think', 'conservative', 'one', 'try', 'rewrite', 'history', 'ðŸ¤”', 'url'], ['@USER', '@USER', '@USER', 'link', 'article', 'show', 'wrong', 'tell', 'libertarian', 'conservative', 'aka', 'right', 'supposedly', 'believe', 'not', 'bigote', 'pattern', 'notice', 'never', 'wrong'], ['@USER', 'grit', 'tooth', 'blonde', 'break', 'shield', 'feeling', 'pump', 'blonde', 'charge', 'get', 'toss', 'dart', 'aim', 'leg', 'cause', 'minor', 'pain', 'tough'], ['@user', 'term', 'parasite', 'add', 'list', 'fuck', 'term', 'include', 'term', 'widget'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'speak', 'ANTIFA', 'BLM', 'say', 'lie'], ['@USER', 'Antifa', 'burn', 'Conservatives', 'house', 'CNN', 'light', 'torch', '&', 'amp', 'throw', 'gas', 'flame'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'want', 'define', 'PUL', 'EASE', 'live', 'MSMedia', 'SHIT', 'SMEAR', 'point', 'Marxine', 'Waters', 'send', 'ANTIFA', 'minion', 'threaten', 'public', 'place', 'learn', 'not', 'F*CK', 'Pavlovian', 'Left', 'think', 'night', 'night'], ['@USER', 'Thank', 'goodness', 'not', 'MN', 'passive', 'aggressive', 'love'], ['@USER', '@USER', 'Ford', 'conservative', 'hate', 'display', 'democratic', 'freedom', 'attack', 'canadian', 'charter', 'right', 'freedom', 'hate', 'minority', 'constitutional', 'right', 'conservativesattackondemocracy', 'canada', 'theCPCisapoliticalhategroup'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'False', 'ðŸ¤£'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'defend', '2nd', 'not', 'NRA', '1st', 'join', 'promote', '&', 'amp', 'educate', 'gun', 'owner', '&', 'amp', 'public', 'gun', 'hypocrisy', 'Amy', 'maybe', 'FYI', 'know', 'lot', 'Liberals', 'member', 'NRA', 'agree'], ['@USER', '@USER', '@USER', 'Misdrawed', 'cause', 'not', 'white', 'blonde', 'answer'], ['@USER', '@USER', '@USER', 'mention', 'Jim', 'seriously', 'medical', 'condition', 'SJW', 'tactic', 'Sargon', 'seriously', 'ANTIFA'], ['@USER', '@USER', 'dem', 'lib', 'real', 'job', 'sure', 'lot', 'free', 'time'], ['@USER', '@USER', '@USER', 'exactly', 'liberal', 'Dems', 'think', 'people', 'right', 'funny', 'pit', 'interest', 'similar', 'nuance', 'criminal', 'defense', 'attorney', 'fight', 'day', 'protect', 'bill', 'right'], ['@USER', 'patient', 'autism', 'Cap'], ['@USER', '@USER', 'mean', 'superman', 'pleaseeeeee', 'ðŸ¼', 'ðŸ¼', 'ðŸ¼', 'ðŸ™', 'ðŸ¼'], ['bunch', 'self', 'aggrandize', 'liberal', 'fancy', 'dress', 'mock', 'Trump', 'fancy', 'dress', 'sound', 'lot', 'like', 'Dems', 'try', 'lynch', 'Kavanaugh', 'politic', 'metoowitchhunt', 'url'], ['@USER', 'generous', 'offer'], ['@user', 'hello', 'twin', 'let', 'ke', 'correct', 'typoness', 'group', 'G)I', 'DLE', 'case', 'lazy', 'use', 'use', 'IDLE', 'instead', 'ðŸ˜‰'], ['@USER', '@USER', '@USER', '@USER', 'Bwahahaha', 'work', 'liberal', 'tool', 'honest', 'work', 'Conservatives', 'depend', 'support', 'chubby', 'little', 'self', 'hear', 'get', 'free', 'food', 'protester'], ['Republicans', 'Conservatives', '@USER', 'wet', 'dream', 'step', 'confirm', 'predator', 'SCOTUS', 'judge', 'Step', 'eliminate', 'woman', 'choice', 'step', 'eliminate', 'non', 'white', 'voting', 'right', 'step', 'eliminate', 'woman', 'voting', 'right', 'step', 'end', 'immigration'], ['wcw', 'look', 'beautiful', 'wish', 'beauty', 'tho', 'love', 'baby', 'sister', 'ðŸ˜', 'ðŸ˜©', 'â¤', 'ï¸', '@user', 'URL'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'say', 'Antifa', 'never', 'oppress', 'Nazis', 'gotcha'], ['@USER', '@USER', 'able', 'reconnect', 'mind', 'try', 'ðŸ¤ž']]\n",
            "['@USER @USER @USER We do know she belongs to the violent liberal Antifa party who w/ their KKK HOODS beat up total strangers &amp; are funded to do so by her party. Most violent &amp; corrupt party trying to now pour on the tears &amp; get us to believe they have a heart. Hard sell to anyone including voters!'\n",
            " '@USER you are a lying corrupt traitor!!! Nobody wants to hear anymore of your lies!!! #DeepStateCorruption URL'\n",
            " '@USER @USER Typical liberals. Principled until it comes to their money.'\n",
            " '@USER Because she knew how to add up the millions in @USER donations received for access?'\n",
            " '@USER Those idiots are fringe right. They are hated by most conservatives.'\n",
            " '@USER @USER @USER I thought conservatives were the ones trying to rewrite history? ðŸ¤” URL'\n",
            " '@USER @USER @USER I linked to an article showing you why you\\'re wrong. You\\'re telling libertarians and conservatives\" aka the right what they supposedly believe. I\\'m not bigoted, it\\'s just a pattern I\\'ve noticed that never seems to be wrong.\"'\n",
            " '@USER Gritting her teeth, the blonde then broke her shield and feeling pumped the blonde charged after him. Give me all you got!\" She then tossed some of her darts, aiming for his legs and would cause only minor pain. \"Show me how tough you are!\"\"'\n",
            " '@USER The term parasite\" has now been added to my list of \"fuck you\" terms, which also includes the term \"widget\".\"'\n",
            " '@USER @USER @USER @USER @USER I just spoke with ANTIFA and BLM. They said you lied.'\n",
            " '@USER Antifa would burn a Conservatives house down and CNN would be there lighting the torches &amp; throwing gas on the flames.'\n",
            " '@USER @USER @USER @USER @USER ...how you want to be defined\"? PUL-EASE! When you live under a 24/7/365 MSMedia SHIT-SMEAR to the point you have Marxine Waters sending her ANTIFA minions out to threaten you in public places - you learn not to give a F*CK what the \"Pavlovian Left\" thinks. Night, night ðŸŒ™!\"'\n",
            " '@USER Thank goodness he is not MN passive aggressive. Love it!!'\n",
            " '@USER @USER Ford and the conservatives hates the displays of democratic freedoms hence their attacks on the Canadian charter of rights and freedoms and their hate for minorities constitutional rights #conservativesattackondemocracy in #canada #theCPCisapoliticalhategroup'\n",
            " '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER False ðŸ¤£'\n",
            " \"@USER @USER @USER @USER @USER 3) I will always defend the 2nd. But not the NRA who when I 1st joined was about promoting &amp; educating gun owners &amp; the public about guns. No hypocrisy here Amy well except maybe on your part. FYI I know a lot of Liberals that a members of the NRA. I don't always agree with them\"\n",
            " '@USER @USER @USER Misdrawed. Cause she is not white and blonde. Why he did that? Can you answer that?'\n",
            " '@USER @USER @USER Not to mention that Jim seriously has a medical condition. What an SJW tactic to take. Sargon seriously has become what ANTIFA is .'\n",
            " '@USER @USER Do any Dems/libs have a real job?? They sure have a lot of free time!!'\n",
            " \"@USER @USER @USER That is exactly what liberals and Dems think about people on the right. It's funny how we've been pitted against each other when really our interests are all very similar with just some nuances. I'm a criminal defense attorney and I fight every day to protect our bill of rights.\"\n",
            " '@USER Be patient with us we have autism\" Cap!\"'\n",
            " '@USER @USER does that mean He still our #superman ? Pleaseeeeee say he is ðŸ™ðŸ¼ðŸ™ðŸ¼ðŸ™ðŸ¼ðŸ™ðŸ¼'\n",
            " '... a bunch of self-aggrandizing liberals in fancy dresses mocking Trump.â€ Except for the fancy dresses, sounds a lot like the Dems who are trying to lynch Kavanaugh... #politics #MeTooWitchhunt URL'\n",
            " '@USER He is so generous with his offers.'\n",
            " '@USER Hello, twin. Let ke correcting your typoness. Our group name is (G)I-DLE in case you are too lazy to use ()\" or \"-\" , you can use IDLE instead ðŸ˜‰\"'\n",
            " '@USER @USER @USER @USER Bwahahaha-work? They are Liberal tools,honest work is for the Conservatives who they depend on to support their chubby little selves. Also,I hear they are getting $50 (and free food) to be protesters.\"\"'\n",
            " \"#Republicans #Conservatives @USER wet dreams: Step 1: Confirm predator as SCOTUS judge Step 2: Eliminate women's choice Step 3: Eliminate non white's voting rights Step 4: Eliminate women's voting rights Step 5: End immigration\"\n",
            " '#wcw because look at how beautiful she is!!!!!! I wish I had this beauty tho but I am just in love with my baby sister ðŸ˜ðŸ˜©â¤ï¸ @USER URL'\n",
            " \"@USER @USER @USER @USER @USER @USER @USER @USER So you're saying Antifa should have never oppressed the Nazis... gotcha.\"\n",
            " '@USER @USER You may be able to reconnect now. Mind trying for us? ðŸ¤ž']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfBanWTdQpQ1",
        "colab_type": "text"
      },
      "source": [
        "# **Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtN-FilEQoqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ystSZoiQzhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t07hLU4Q18f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_loss(actual, predicted):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWkFh8tJjeyP",
        "colab_type": "text"
      },
      "source": [
        "# **Save data to file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXcp_1yTjcEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(training_x, columns = ['text_a'])\n",
        "train['label'] = training_y\n",
        "test = pd.DataFrame(test_x, columns = ['text_a'])\n",
        "test['label'] = test_y\n",
        "val = pd.DataFrame(validation_x, columns = ['text_a'])\n",
        "val['label'] = validation_y\n",
        "train.to_csv('train.tsv', index=False, sep='\\t')\n",
        "test.to_csv('test.tsv', index=False, sep='\\t')\n",
        "val.to_csv('val.tsv', index=False, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96_du6DZpQJs",
        "colab_type": "text"
      },
      "source": [
        "# Download model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XSNataMlwKX",
        "colab_type": "code",
        "outputId": "d0c829be-6838-48c8-9345-5c8e37287271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/PaddlePaddle/ERNIE.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ERNIE'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 1392 (delta 0), reused 0 (delta 0), pack-reused 1386\u001b[K\n",
            "Receiving objects: 100% (1392/1392), 16.05 MiB | 18.16 MiB/s, done.\n",
            "Resolving deltas: 100% (802/802), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eo0e8FUpPHs",
        "colab_type": "code",
        "outputId": "92b9ae5b-e213-4631-b50b-06f2aac2da35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://ernie.bj.bcebos.com/ERNIE_Base_en_stable-2.0.0.tar.gz\n",
        "!gunzip ERNIE_Base_en_stable-2.0.0.tar.gz\n",
        "!tar -xvf ERNIE_Base_en_stable-2.0.0.tar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-23 14:02:39--  https://ernie.bj.bcebos.com/ERNIE_Base_en_stable-2.0.0.tar.gz\n",
            "Resolving ernie.bj.bcebos.com (ernie.bj.bcebos.com)... 103.235.46.61\n",
            "Connecting to ernie.bj.bcebos.com (ernie.bj.bcebos.com)|103.235.46.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405413980 (387M) [application/x-gzip]\n",
            "Saving to: â€˜ERNIE_Base_en_stable-2.0.0.tar.gzâ€™\n",
            "\n",
            "ERNIE_Base_en_stabl 100%[===================>] 386.63M  16.8MB/s    in 40s     \n",
            "\n",
            "2020-02-23 14:03:22 (9.61 MB/s) - â€˜ERNIE_Base_en_stable-2.0.0.tar.gzâ€™ saved [405413980/405413980]\n",
            "\n",
            "ernie_config.json\n",
            "params/\n",
            "params/encoder_layer_4_post_att_layer_norm_scale\n",
            "params/encoder_layer_2_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_2_ffn_fc_1.w_0\n",
            "params/encoder_layer_6_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_7_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_11_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_5_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_1_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_11_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_7_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_1_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_9_ffn_fc_1.b_0\n",
            "params/encoder_layer_4_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_2_ffn_fc_1.b_0\n",
            "params/encoder_layer_7_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_7_post_att_layer_norm_scale\n",
            "params/encoder_layer_6_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_10_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_5_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_3_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_1_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_10_ffn_fc_0.w_0\n",
            "params/encoder_layer_6_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_3_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_8_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_4_post_att_layer_norm_bias\n",
            "params/pre_encoder_layer_norm_scale\n",
            "params/encoder_layer_2_post_att_layer_norm_scale\n",
            "params/encoder_layer_5_post_att_layer_norm_bias\n",
            "params/encoder_layer_4_ffn_fc_0.b_0\n",
            "params/encoder_layer_4_ffn_fc_1.b_0\n",
            "params/encoder_layer_10_ffn_fc_1.w_0\n",
            "params/encoder_layer_2_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_3_post_att_layer_norm_bias\n",
            "params/encoder_layer_1_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_0_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_8_ffn_fc_1.b_0\n",
            "params/encoder_layer_3_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_2_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_6_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_6_post_att_layer_norm_scale\n",
            "params/encoder_layer_0_post_att_layer_norm_scale\n",
            "params/encoder_layer_2_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_5_ffn_fc_1.w_0\n",
            "params/encoder_layer_0_ffn_fc_0.b_0\n",
            "params/encoder_layer_1_post_att_layer_norm_scale\n",
            "params/encoder_layer_6_post_att_layer_norm_bias\n",
            "params/encoder_layer_4_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_5_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_11_post_att_layer_norm_scale\n",
            "params/encoder_layer_9_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_9_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_0_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_2_ffn_fc_0.w_0\n",
            "params/pos_embedding\n",
            "params/encoder_layer_8_ffn_fc_0.b_0\n",
            "params/encoder_layer_11_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_0_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_9_post_att_layer_norm_bias\n",
            "params/encoder_layer_2_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_5_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_9_multi_head_att_key_fc.b_0\n",
            "params/task_embedding\n",
            "params/encoder_layer_10_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_4_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_7_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_7_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_0_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_10_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_1_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_5_post_att_layer_norm_scale\n",
            "params/encoder_layer_8_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_0_ffn_fc_0.w_0\n",
            "params/encoder_layer_5_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_7_ffn_fc_0.w_0\n",
            "params/encoder_layer_7_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_7_ffn_fc_1.w_0\n",
            "params/encoder_layer_2_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_9_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_9_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_9_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_4_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_11_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_5_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_1_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_0_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_6_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_7_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_8_ffn_fc_1.w_0\n",
            "params/encoder_layer_9_ffn_fc_1.w_0\n",
            "params/sent_embedding\n",
            "params/encoder_layer_0_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_10_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_6_ffn_fc_0.w_0\n",
            "params/encoder_layer_1_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_5_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_6_ffn_fc_1.b_0\n",
            "params/encoder_layer_0_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_5_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_2_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_0_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_9_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_1_ffn_fc_1.w_0\n",
            "params/encoder_layer_0_ffn_fc_1.w_0\n",
            "params/encoder_layer_8_ffn_fc_0.w_0\n",
            "params/encoder_layer_11_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_5_ffn_fc_0.w_0\n",
            "params/pre_encoder_layer_norm_bias\n",
            "params/encoder_layer_6_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_10_ffn_fc_0.b_0\n",
            "params/encoder_layer_6_ffn_fc_1.w_0\n",
            "params/encoder_layer_4_ffn_fc_1.w_0\n",
            "params/encoder_layer_2_ffn_fc_0.b_0\n",
            "params/encoder_layer_5_ffn_fc_0.b_0\n",
            "params/encoder_layer_10_ffn_fc_1.b_0\n",
            "params/pooled_fc.w_0\n",
            "params/encoder_layer_7_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_8_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_3_ffn_fc_0.b_0\n",
            "params/encoder_layer_11_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_7_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_9_ffn_fc_0.w_0\n",
            "params/encoder_layer_9_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_0_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_7_post_att_layer_norm_bias\n",
            "params/encoder_layer_3_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_11_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_5_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_0_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_4_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_3_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_3_multi_head_att_query_fc.w_0\n",
            "params/encoder_layer_11_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_8_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_4_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_6_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_9_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_3_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_6_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_9_ffn_fc_0.b_0\n",
            "params/encoder_layer_10_post_att_layer_norm_scale\n",
            "params/encoder_layer_11_ffn_fc_0.b_0\n",
            "params/pooled_fc.b_0\n",
            "params/encoder_layer_2_post_att_layer_norm_bias\n",
            "params/encoder_layer_8_multi_head_att_output_fc.w_0\n",
            "params/encoder_layer_1_ffn_fc_0.w_0\n",
            "params/encoder_layer_4_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_8_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_10_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_6_ffn_fc_0.b_0\n",
            "params/encoder_layer_5_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_3_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_3_ffn_fc_1.b_0\n",
            "params/encoder_layer_10_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_10_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_8_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_8_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_11_post_att_layer_norm_bias\n",
            "params/encoder_layer_8_post_att_layer_norm_scale\n",
            "params/encoder_layer_10_post_att_layer_norm_bias\n",
            "params/encoder_layer_4_ffn_fc_0.w_0\n",
            "params/encoder_layer_1_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_4_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_11_ffn_fc_0.w_0\n",
            "params/encoder_layer_3_post_att_layer_norm_scale\n",
            "params/encoder_layer_10_post_ffn_layer_norm_bias\n",
            "params/encoder_layer_11_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_10_multi_head_att_query_fc.b_0\n",
            "params/encoder_layer_6_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_8_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_0_ffn_fc_1.b_0\n",
            "params/encoder_layer_9_post_att_layer_norm_scale\n",
            "params/word_embedding\n",
            "params/encoder_layer_3_ffn_fc_1.w_0\n",
            "params/encoder_layer_11_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_3_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_11_ffn_fc_1.b_0\n",
            "params/encoder_layer_2_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_3_ffn_fc_0.w_0\n",
            "params/encoder_layer_5_ffn_fc_1.b_0\n",
            "params/encoder_layer_1_ffn_fc_0.b_0\n",
            "params/encoder_layer_1_ffn_fc_1.b_0\n",
            "params/encoder_layer_8_post_att_layer_norm_bias\n",
            "params/encoder_layer_7_ffn_fc_0.b_0\n",
            "params/encoder_layer_1_multi_head_att_value_fc.b_0\n",
            "params/encoder_layer_2_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_7_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_3_post_ffn_layer_norm_scale\n",
            "params/encoder_layer_11_ffn_fc_1.w_0\n",
            "params/encoder_layer_1_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_0_post_att_layer_norm_bias\n",
            "params/encoder_layer_4_multi_head_att_key_fc.b_0\n",
            "params/encoder_layer_2_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_4_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_8_multi_head_att_key_fc.w_0\n",
            "params/encoder_layer_10_multi_head_att_output_fc.b_0\n",
            "params/encoder_layer_7_ffn_fc_1.b_0\n",
            "params/encoder_layer_1_post_att_layer_norm_bias\n",
            "params/encoder_layer_9_multi_head_att_value_fc.w_0\n",
            "params/encoder_layer_6_post_ffn_layer_norm_bias\n",
            "vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvYrn4PUpwOQ",
        "colab_type": "code",
        "outputId": "c8d8a6cc-b691-4332-be35-22d583a78ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir -p parameters/params\n",
        "!mkdir -p 'dataset/sem-eval'\n",
        "!mv train.tsv dataset/sem-eval/\n",
        "!mv test.tsv dataset/sem-eval/\n",
        "!mv val.tsv dataset/sem-eval/\n",
        "!mv params/ parameters/params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move 'params/' to 'parameters/params/params': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8m9ZA3sp4GH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv dataset/ ERNIE/\n",
        "!mv parameters/ ERNIE/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9CIul0nKj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('ERNIE/')\n",
        "os.environ['TASK_DATA_PATH']='dataset'\n",
        "os.environ['MODEL_PATH']='parameters/params'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeF7dJZvs2cT",
        "colab_type": "text"
      },
      "source": [
        "# Run classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNZ7B3gJs9IH",
        "colab_type": "code",
        "outputId": "c3f069ad-2232-4a43-c593-2e5006b7e11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sh '/content/gdrive/My Drive/Pracownia/Ernie/sem-eval-task.sh'"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Pracownia/Ernie/sem-eval-task.sh: 8: /content/gdrive/My Drive/Pracownia/Ernie/sem-eval-task.sh: [[: not found\n",
            "2020-02-23 14:06:54,059-INFO: -----------  Configuration Arguments -----------\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   68]:\t-----------  Configuration Arguments -----------\n",
            "2020-02-23 14:06:54,059-INFO: batch_size: 32\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tbatch_size: 32\n",
            "2020-02-23 14:06:54,059-INFO: checkpoints: ./checkpoints\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tcheckpoints: ./checkpoints\n",
            "2020-02-23 14:06:54,059-INFO: chunk_scheme: IOB\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tchunk_scheme: IOB\n",
            "2020-02-23 14:06:54,059-INFO: decr_every_n_nan_or_inf: 2\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdecr_every_n_nan_or_inf: 2\n",
            "2020-02-23 14:06:54,059-INFO: decr_ratio: 0.8\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdecr_ratio: 0.8\n",
            "2020-02-23 14:06:54,059-INFO: dev_set: dataset/sem-eval/val.tsv\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdev_set: dataset/sem-eval/val.tsv\n",
            "2020-02-23 14:06:54,059-INFO: diagnostic: None\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdiagnostic: None\n",
            "2020-02-23 14:06:54,059-INFO: diagnostic_save: None\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdiagnostic_save: None\n",
            "2020-02-23 14:06:54,059-INFO: do_lower_case: True\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdo_lower_case: True\n",
            "2020-02-23 14:06:54,059-INFO: do_test: True\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdo_test: True\n",
            "2020-02-23 14:06:54,059-INFO: do_train: True\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdo_train: True\n",
            "2020-02-23 14:06:54,059-INFO: do_val: True\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdo_val: True\n",
            "2020-02-23 14:06:54,059-INFO: doc_stride: 128\n",
            "[INFO] 2020-02-23 14:06:54,059 [     args.py:   70]:\tdoc_stride: 128\n",
            "2020-02-23 14:06:54,060-INFO: enable_ce: False\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tenable_ce: False\n",
            "2020-02-23 14:06:54,060-INFO: epoch: 3\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tepoch: 3\n",
            "2020-02-23 14:06:54,060-INFO: ernie_config_path: script/en_glue/ernie_base/ernie_config.json\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\ternie_config_path: script/en_glue/ernie_base/ernie_config.json\n",
            "2020-02-23 14:06:54,060-INFO: for_cn: False\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tfor_cn: False\n",
            "2020-02-23 14:06:54,060-INFO: in_tokens: False\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tin_tokens: False\n",
            "2020-02-23 14:06:54,060-INFO: incr_every_n_steps: 100\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tincr_every_n_steps: 100\n",
            "2020-02-23 14:06:54,060-INFO: incr_ratio: 2.0\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tincr_ratio: 2.0\n",
            "2020-02-23 14:06:54,060-INFO: init_checkpoint: None\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tinit_checkpoint: None\n",
            "2020-02-23 14:06:54,060-INFO: init_loss_scaling: 102400\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tinit_loss_scaling: 102400\n",
            "2020-02-23 14:06:54,060-INFO: init_pretraining_params: parameters/params/params\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tinit_pretraining_params: parameters/params/params\n",
            "2020-02-23 14:06:54,060-INFO: is_classify: True\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tis_classify: True\n",
            "2020-02-23 14:06:54,060-INFO: is_distributed: False\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tis_distributed: False\n",
            "2020-02-23 14:06:54,060-INFO: is_regression: False\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tis_regression: False\n",
            "2020-02-23 14:06:54,060-INFO: label_map_config: None\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tlabel_map_config: None\n",
            "2020-02-23 14:06:54,060-INFO: learning_rate: 2e-05\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tlearning_rate: 2e-05\n",
            "2020-02-23 14:06:54,060-INFO: lr_scheduler: linear_warmup_decay\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tlr_scheduler: linear_warmup_decay\n",
            "2020-02-23 14:06:54,060-INFO: max_answer_length: 100\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tmax_answer_length: 100\n",
            "2020-02-23 14:06:54,060-INFO: max_query_length: 64\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tmax_query_length: 64\n",
            "2020-02-23 14:06:54,060-INFO: max_seq_len: 128\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tmax_seq_len: 128\n",
            "2020-02-23 14:06:54,060-INFO: metric: simple_accuracy\n",
            "[INFO] 2020-02-23 14:06:54,060 [     args.py:   70]:\tmetric: simple_accuracy\n",
            "2020-02-23 14:06:54,061-INFO: metrics: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tmetrics: True\n",
            "2020-02-23 14:06:54,061-INFO: n_best_size: 20\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tn_best_size: 20\n",
            "2020-02-23 14:06:54,061-INFO: num_iteration_per_drop_scope: 1\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tnum_iteration_per_drop_scope: 1\n",
            "2020-02-23 14:06:54,061-INFO: num_labels: 2\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tnum_labels: 2\n",
            "2020-02-23 14:06:54,061-INFO: predict_batch_size: None\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tpredict_batch_size: None\n",
            "2020-02-23 14:06:54,061-INFO: random_seed: 1\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\trandom_seed: 1\n",
            "2020-02-23 14:06:54,061-INFO: save_steps: 10000\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tsave_steps: 10000\n",
            "2020-02-23 14:06:54,061-INFO: shuffle: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tshuffle: True\n",
            "2020-02-23 14:06:54,061-INFO: skip_steps: 10\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tskip_steps: 10\n",
            "2020-02-23 14:06:54,061-INFO: task_id: 0\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\ttask_id: 0\n",
            "2020-02-23 14:06:54,061-INFO: test_save: output/test_out.{1..5}.2e-5.32.3.tsv\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\ttest_save: output/test_out.{1..5}.2e-5.32.3.tsv\n",
            "2020-02-23 14:06:54,061-INFO: test_set: dataset/sem-eval/test.tsv\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\ttest_set: dataset/sem-eval/test.tsv\n",
            "2020-02-23 14:06:54,061-INFO: tokenizer: FullTokenizer\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\ttokenizer: FullTokenizer\n",
            "2020-02-23 14:06:54,061-INFO: train_set: dataset/sem-eval/train.tsv\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\ttrain_set: dataset/sem-eval/train.tsv\n",
            "2020-02-23 14:06:54,061-INFO: use_cuda: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tuse_cuda: True\n",
            "2020-02-23 14:06:54,061-INFO: use_dynamic_loss_scaling: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tuse_dynamic_loss_scaling: True\n",
            "2020-02-23 14:06:54,061-INFO: use_fast_executor: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tuse_fast_executor: True\n",
            "2020-02-23 14:06:54,061-INFO: use_fp16: False\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tuse_fp16: False\n",
            "2020-02-23 14:06:54,061-INFO: use_multi_gpu_test: False\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tuse_multi_gpu_test: False\n",
            "2020-02-23 14:06:54,061-INFO: validation_steps: 800000000000\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tvalidation_steps: 800000000000\n",
            "2020-02-23 14:06:54,061-INFO: verbose: True\n",
            "[INFO] 2020-02-23 14:06:54,061 [     args.py:   70]:\tverbose: True\n",
            "2020-02-23 14:06:54,062-INFO: vocab_path: script/en_glue/ernie_base/vocab.txt\n",
            "[INFO] 2020-02-23 14:06:54,062 [     args.py:   70]:\tvocab_path: script/en_glue/ernie_base/vocab.txt\n",
            "2020-02-23 14:06:54,062-INFO: warmup_proportion: 0.1\n",
            "[INFO] 2020-02-23 14:06:54,062 [     args.py:   70]:\twarmup_proportion: 0.1\n",
            "2020-02-23 14:06:54,062-INFO: weight_decay: 0.0\n",
            "[INFO] 2020-02-23 14:06:54,062 [     args.py:   70]:\tweight_decay: 0.0\n",
            "2020-02-23 14:06:54,062-INFO: ------------------------------------------------\n",
            "[INFO] 2020-02-23 14:06:54,062 [     args.py:   71]:\t------------------------------------------------\n",
            "2020-02-23 14:06:54,062-INFO: attention_probs_dropout_prob: 0.1\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tattention_probs_dropout_prob: 0.1\n",
            "2020-02-23 14:06:54,062-INFO: hidden_act: gelu\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\thidden_act: gelu\n",
            "2020-02-23 14:06:54,062-INFO: hidden_dropout_prob: 0.1\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\thidden_dropout_prob: 0.1\n",
            "2020-02-23 14:06:54,062-INFO: hidden_size: 768\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\thidden_size: 768\n",
            "2020-02-23 14:06:54,062-INFO: initializer_range: 0.02\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tinitializer_range: 0.02\n",
            "2020-02-23 14:06:54,062-INFO: max_position_embeddings: 512\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tmax_position_embeddings: 512\n",
            "2020-02-23 14:06:54,062-INFO: num_attention_heads: 12\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tnum_attention_heads: 12\n",
            "2020-02-23 14:06:54,062-INFO: num_hidden_layers: 12\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tnum_hidden_layers: 12\n",
            "2020-02-23 14:06:54,062-INFO: sent_type_vocab_size: 4\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tsent_type_vocab_size: 4\n",
            "2020-02-23 14:06:54,062-INFO: task_type_vocab_size: 16\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\ttask_type_vocab_size: 16\n",
            "2020-02-23 14:06:54,062-INFO: vocab_size: 30522\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   52]:\tvocab_size: 30522\n",
            "2020-02-23 14:06:54,062-INFO: ------------------------------------------------\n",
            "[INFO] 2020-02-23 14:06:54,062 [    ernie.py:   53]:\t------------------------------------------------\n",
            "2020-02-23 14:06:54,159-INFO: Device count: 1\n",
            "[INFO] 2020-02-23 14:06:54,159 [run_classifier.py:  103]:\tDevice count: 1\n",
            "2020-02-23 14:06:54,159-INFO: Num train examples: 11916\n",
            "[INFO] 2020-02-23 14:06:54,159 [run_classifier.py:  104]:\tNum train examples: 11916\n",
            "2020-02-23 14:06:54,159-INFO: Max train steps: 1117\n",
            "[INFO] 2020-02-23 14:06:54,159 [run_classifier.py:  105]:\tMax train steps: 1117\n",
            "2020-02-23 14:06:54,159-INFO: Num warmup steps: 111\n",
            "[INFO] 2020-02-23 14:06:54,159 [run_classifier.py:  106]:\tNum warmup steps: 111\n",
            "2020-02-23 14:06:55,272-INFO: Theoretical memory usage in training: 17039.884 - 17851.307 MB\n",
            "[INFO] 2020-02-23 14:06:55,272 [run_classifier.py:  146]:\tTheoretical memory usage in training: 17039.884 - 17851.307 MB\n",
            "W0223 14:06:56.821655   889 device_context.cc:236] Please NOTE: device: 0, CUDA Capability: 60, Driver API Version: 10.1, Runtime API Version: 10.0\n",
            "W0223 14:06:56.825812   889 device_context.cc:244] device: 0, cuDNN Version: 7.6.\n",
            "2020-02-23 14:06:58,401-INFO: Load pretraining parameters from parameters/params/params.\n",
            "[INFO] 2020-02-23 14:06:58,401 [     init.py:   88]:\tLoad pretraining parameters from parameters/params/params.\n",
            "I0223 14:06:58.480172   889 parallel_executor.cc:421] The number of CUDAPlace, which is used in ParallelExecutor, is 1. And the Program will be copied 1 copies\n",
            "I0223 14:06:58.523731   889 build_strategy.cc:363] SeqOnlyAllReduceOps:0, num_trainers:1\n",
            "I0223 14:06:58.592207   889 parallel_executor.cc:285] Inplace strategy is enabled, when build_strategy.enable_inplace = True\n",
            "I0223 14:06:58.622802   889 parallel_executor.cc:368] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0\n",
            "2020-02-23 14:07:01,779-INFO: train pyreader queue size: 68, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:07:01,779 [run_classifier.py:  275]:\ttrain pyreader queue size: 68, learning rate: 0.000002\n",
            "2020-02-23 14:07:01,790-INFO: epoch: 0, progress: 2558/11916, step: 10, ave loss: 0.688782, ave acc: 0.562500, speed: 2.980021 steps/s\n",
            "[INFO] 2020-02-23 14:07:01,790 [run_classifier.py:  287]:\tepoch: 0, progress: 2558/11916, step: 10, ave loss: 0.688782, ave acc: 0.562500, speed: 2.980021 steps/s\n",
            "2020-02-23 14:07:05,059-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:07:05,059 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:07:05,059-INFO: epoch: 0, progress: 2944/11916, step: 20, ave loss: 0.671504, ave acc: 0.625000, speed: 3.066813 steps/s\n",
            "[INFO] 2020-02-23 14:07:05,059 [run_classifier.py:  287]:\tepoch: 0, progress: 2944/11916, step: 20, ave loss: 0.671504, ave acc: 0.625000, speed: 3.066813 steps/s\n",
            "2020-02-23 14:07:08,554-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:07:08,554 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:07:08,554-INFO: epoch: 0, progress: 3264/11916, step: 30, ave loss: 0.627489, ave acc: 0.656250, speed: 2.860972 steps/s\n",
            "[INFO] 2020-02-23 14:07:08,554 [run_classifier.py:  287]:\tepoch: 0, progress: 3264/11916, step: 30, ave loss: 0.627489, ave acc: 0.656250, speed: 2.860972 steps/s\n",
            "2020-02-23 14:07:11,562-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:07:11,562 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:07:11,562-INFO: epoch: 0, progress: 3584/11916, step: 40, ave loss: 0.607549, ave acc: 0.625000, speed: 3.325043 steps/s\n",
            "[INFO] 2020-02-23 14:07:11,562 [run_classifier.py:  287]:\tepoch: 0, progress: 3584/11916, step: 40, ave loss: 0.607549, ave acc: 0.625000, speed: 3.325043 steps/s\n",
            "2020-02-23 14:07:14,568-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:07:14,568 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:07:14,568-INFO: epoch: 0, progress: 3904/11916, step: 50, ave loss: 0.709715, ave acc: 0.562500, speed: 3.326552 steps/s\n",
            "[INFO] 2020-02-23 14:07:14,568 [run_classifier.py:  287]:\tepoch: 0, progress: 3904/11916, step: 50, ave loss: 0.709715, ave acc: 0.562500, speed: 3.326552 steps/s\n",
            "2020-02-23 14:07:17,615-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:07:17,615 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:07:17,616-INFO: epoch: 0, progress: 4224/11916, step: 60, ave loss: 0.592789, ave acc: 0.750000, speed: 3.281717 steps/s\n",
            "[INFO] 2020-02-23 14:07:17,616 [run_classifier.py:  287]:\tepoch: 0, progress: 4224/11916, step: 60, ave loss: 0.592789, ave acc: 0.750000, speed: 3.281717 steps/s\n",
            "2020-02-23 14:07:20,753-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:07:20,753 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:07:20,753-INFO: epoch: 0, progress: 4544/11916, step: 70, ave loss: 0.609872, ave acc: 0.656250, speed: 3.187715 steps/s\n",
            "[INFO] 2020-02-23 14:07:20,753 [run_classifier.py:  287]:\tepoch: 0, progress: 4544/11916, step: 70, ave loss: 0.609872, ave acc: 0.656250, speed: 3.187715 steps/s\n",
            "2020-02-23 14:07:23,762-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:07:23,762 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:07:23,762-INFO: epoch: 0, progress: 4864/11916, step: 80, ave loss: 0.589620, ave acc: 0.718750, speed: 3.323066 steps/s\n",
            "[INFO] 2020-02-23 14:07:23,762 [run_classifier.py:  287]:\tepoch: 0, progress: 4864/11916, step: 80, ave loss: 0.589620, ave acc: 0.718750, speed: 3.323066 steps/s\n",
            "2020-02-23 14:07:26,854-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:07:26,854 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:07:26,854-INFO: epoch: 0, progress: 5184/11916, step: 90, ave loss: 0.482735, ave acc: 0.812500, speed: 3.234668 steps/s\n",
            "[INFO] 2020-02-23 14:07:26,854 [run_classifier.py:  287]:\tepoch: 0, progress: 5184/11916, step: 90, ave loss: 0.482735, ave acc: 0.812500, speed: 3.234668 steps/s\n",
            "2020-02-23 14:07:29,952-INFO: train pyreader queue size: 70, learning rate: 0.000018\n",
            "[INFO] 2020-02-23 14:07:29,952 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000018\n",
            "2020-02-23 14:07:29,952-INFO: epoch: 0, progress: 5504/11916, step: 100, ave loss: 0.440220, ave acc: 0.875000, speed: 3.228116 steps/s\n",
            "[INFO] 2020-02-23 14:07:29,952 [run_classifier.py:  287]:\tepoch: 0, progress: 5504/11916, step: 100, ave loss: 0.440220, ave acc: 0.875000, speed: 3.228116 steps/s\n",
            "2020-02-23 14:07:32,999-INFO: train pyreader queue size: 70, learning rate: 0.000020\n",
            "[INFO] 2020-02-23 14:07:32,999 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000020\n",
            "2020-02-23 14:07:32,999-INFO: epoch: 0, progress: 5824/11916, step: 110, ave loss: 0.496664, ave acc: 0.750000, speed: 3.282197 steps/s\n",
            "[INFO] 2020-02-23 14:07:32,999 [run_classifier.py:  287]:\tepoch: 0, progress: 5824/11916, step: 110, ave loss: 0.496664, ave acc: 0.750000, speed: 3.282197 steps/s\n",
            "2020-02-23 14:07:36,119-INFO: train pyreader queue size: 70, learning rate: 0.000018\n",
            "[INFO] 2020-02-23 14:07:36,119 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000018\n",
            "2020-02-23 14:07:36,120-INFO: epoch: 0, progress: 6144/11916, step: 120, ave loss: 0.346669, ave acc: 0.875000, speed: 3.204431 steps/s\n",
            "[INFO] 2020-02-23 14:07:36,120 [run_classifier.py:  287]:\tepoch: 0, progress: 6144/11916, step: 120, ave loss: 0.346669, ave acc: 0.875000, speed: 3.204431 steps/s\n",
            "2020-02-23 14:07:39,238-INFO: train pyreader queue size: 70, learning rate: 0.000018\n",
            "[INFO] 2020-02-23 14:07:39,238 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000018\n",
            "2020-02-23 14:07:39,239-INFO: epoch: 0, progress: 6464/11916, step: 130, ave loss: 0.335754, ave acc: 0.875000, speed: 3.206335 steps/s\n",
            "[INFO] 2020-02-23 14:07:39,239 [run_classifier.py:  287]:\tepoch: 0, progress: 6464/11916, step: 130, ave loss: 0.335754, ave acc: 0.875000, speed: 3.206335 steps/s\n",
            "2020-02-23 14:07:42,212-INFO: train pyreader queue size: 70, learning rate: 0.000018\n",
            "[INFO] 2020-02-23 14:07:42,212 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000018\n",
            "2020-02-23 14:07:42,212-INFO: epoch: 0, progress: 6784/11916, step: 140, ave loss: 0.504238, ave acc: 0.812500, speed: 3.363536 steps/s\n",
            "[INFO] 2020-02-23 14:07:42,212 [run_classifier.py:  287]:\tepoch: 0, progress: 6784/11916, step: 140, ave loss: 0.504238, ave acc: 0.812500, speed: 3.363536 steps/s\n",
            "2020-02-23 14:07:45,335-INFO: train pyreader queue size: 70, learning rate: 0.000017\n",
            "[INFO] 2020-02-23 14:07:45,335 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000017\n",
            "2020-02-23 14:07:45,335-INFO: epoch: 0, progress: 7104/11916, step: 150, ave loss: 0.513209, ave acc: 0.718750, speed: 3.201705 steps/s\n",
            "[INFO] 2020-02-23 14:07:45,335 [run_classifier.py:  287]:\tepoch: 0, progress: 7104/11916, step: 150, ave loss: 0.513209, ave acc: 0.718750, speed: 3.201705 steps/s\n",
            "2020-02-23 14:07:48,515-INFO: train pyreader queue size: 70, learning rate: 0.000017\n",
            "[INFO] 2020-02-23 14:07:48,515 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000017\n",
            "2020-02-23 14:07:48,516-INFO: epoch: 0, progress: 7424/11916, step: 160, ave loss: 0.459218, ave acc: 0.781250, speed: 3.144508 steps/s\n",
            "[INFO] 2020-02-23 14:07:48,516 [run_classifier.py:  287]:\tepoch: 0, progress: 7424/11916, step: 160, ave loss: 0.459218, ave acc: 0.781250, speed: 3.144508 steps/s\n",
            "2020-02-23 14:07:51,583-INFO: train pyreader queue size: 70, learning rate: 0.000017\n",
            "[INFO] 2020-02-23 14:07:51,583 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000017\n",
            "2020-02-23 14:07:51,583-INFO: epoch: 0, progress: 7744/11916, step: 170, ave loss: 0.370039, ave acc: 0.812500, speed: 3.260067 steps/s\n",
            "[INFO] 2020-02-23 14:07:51,583 [run_classifier.py:  287]:\tepoch: 0, progress: 7744/11916, step: 170, ave loss: 0.370039, ave acc: 0.812500, speed: 3.260067 steps/s\n",
            "2020-02-23 14:07:54,760-INFO: train pyreader queue size: 70, learning rate: 0.000017\n",
            "[INFO] 2020-02-23 14:07:54,760 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000017\n",
            "2020-02-23 14:07:54,761-INFO: epoch: 0, progress: 8064/11916, step: 180, ave loss: 0.509244, ave acc: 0.781250, speed: 3.147069 steps/s\n",
            "[INFO] 2020-02-23 14:07:54,761 [run_classifier.py:  287]:\tepoch: 0, progress: 8064/11916, step: 180, ave loss: 0.509244, ave acc: 0.781250, speed: 3.147069 steps/s\n",
            "2020-02-23 14:07:57,981-INFO: train pyreader queue size: 70, learning rate: 0.000017\n",
            "[INFO] 2020-02-23 14:07:57,981 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000017\n",
            "2020-02-23 14:07:57,982-INFO: epoch: 0, progress: 8384/11916, step: 190, ave loss: 0.461750, ave acc: 0.718750, speed: 3.104752 steps/s\n",
            "[INFO] 2020-02-23 14:07:57,982 [run_classifier.py:  287]:\tepoch: 0, progress: 8384/11916, step: 190, ave loss: 0.461750, ave acc: 0.718750, speed: 3.104752 steps/s\n",
            "2020-02-23 14:08:01,689-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:01,689 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:01,689-INFO: epoch: 0, progress: 8704/11916, step: 200, ave loss: 0.577847, ave acc: 0.687500, speed: 2.697564 steps/s\n",
            "[INFO] 2020-02-23 14:08:01,689 [run_classifier.py:  287]:\tepoch: 0, progress: 8704/11916, step: 200, ave loss: 0.577847, ave acc: 0.687500, speed: 2.697564 steps/s\n",
            "2020-02-23 14:08:04,699-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:04,699 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:04,699-INFO: epoch: 0, progress: 9024/11916, step: 210, ave loss: 0.349850, ave acc: 0.812500, speed: 3.322152 steps/s\n",
            "[INFO] 2020-02-23 14:08:04,699 [run_classifier.py:  287]:\tepoch: 0, progress: 9024/11916, step: 210, ave loss: 0.349850, ave acc: 0.812500, speed: 3.322152 steps/s\n",
            "2020-02-23 14:08:07,806-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:07,806 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:07,806-INFO: epoch: 0, progress: 9344/11916, step: 220, ave loss: 0.419912, ave acc: 0.781250, speed: 3.218901 steps/s\n",
            "[INFO] 2020-02-23 14:08:07,806 [run_classifier.py:  287]:\tepoch: 0, progress: 9344/11916, step: 220, ave loss: 0.419912, ave acc: 0.781250, speed: 3.218901 steps/s\n",
            "2020-02-23 14:08:11,155-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:11,155 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:11,155-INFO: epoch: 0, progress: 9664/11916, step: 230, ave loss: 0.476767, ave acc: 0.781250, speed: 2.985671 steps/s\n",
            "[INFO] 2020-02-23 14:08:11,155 [run_classifier.py:  287]:\tepoch: 0, progress: 9664/11916, step: 230, ave loss: 0.476767, ave acc: 0.781250, speed: 2.985671 steps/s\n",
            "2020-02-23 14:08:14,405-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:14,405 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:14,406-INFO: epoch: 0, progress: 9984/11916, step: 240, ave loss: 0.532985, ave acc: 0.718750, speed: 3.076892 steps/s\n",
            "[INFO] 2020-02-23 14:08:14,406 [run_classifier.py:  287]:\tepoch: 0, progress: 9984/11916, step: 240, ave loss: 0.532985, ave acc: 0.718750, speed: 3.076892 steps/s\n",
            "2020-02-23 14:08:17,426-INFO: train pyreader queue size: 70, learning rate: 0.000016\n",
            "[INFO] 2020-02-23 14:08:17,426 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000016\n",
            "2020-02-23 14:08:17,427-INFO: epoch: 0, progress: 10304/11916, step: 250, ave loss: 0.317667, ave acc: 0.937500, speed: 3.310098 steps/s\n",
            "[INFO] 2020-02-23 14:08:17,427 [run_classifier.py:  287]:\tepoch: 0, progress: 10304/11916, step: 250, ave loss: 0.317667, ave acc: 0.937500, speed: 3.310098 steps/s\n",
            "2020-02-23 14:08:20,379-INFO: train pyreader queue size: 70, learning rate: 0.000015\n",
            "[INFO] 2020-02-23 14:08:20,379 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000015\n",
            "2020-02-23 14:08:20,379-INFO: epoch: 0, progress: 10624/11916, step: 260, ave loss: 0.315654, ave acc: 0.875000, speed: 3.386906 steps/s\n",
            "[INFO] 2020-02-23 14:08:20,379 [run_classifier.py:  287]:\tepoch: 0, progress: 10624/11916, step: 260, ave loss: 0.315654, ave acc: 0.875000, speed: 3.386906 steps/s\n",
            "2020-02-23 14:08:23,669-INFO: train pyreader queue size: 70, learning rate: 0.000015\n",
            "[INFO] 2020-02-23 14:08:23,669 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000015\n",
            "2020-02-23 14:08:23,669-INFO: epoch: 0, progress: 10944/11916, step: 270, ave loss: 0.324490, ave acc: 0.906250, speed: 3.039711 steps/s\n",
            "[INFO] 2020-02-23 14:08:23,669 [run_classifier.py:  287]:\tepoch: 0, progress: 10944/11916, step: 270, ave loss: 0.324490, ave acc: 0.906250, speed: 3.039711 steps/s\n",
            "2020-02-23 14:08:26,661-INFO: train pyreader queue size: 70, learning rate: 0.000015\n",
            "[INFO] 2020-02-23 14:08:26,661 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000015\n",
            "2020-02-23 14:08:26,662-INFO: epoch: 0, progress: 11264/11916, step: 280, ave loss: 0.291265, ave acc: 0.906250, speed: 3.342159 steps/s\n",
            "[INFO] 2020-02-23 14:08:26,662 [run_classifier.py:  287]:\tepoch: 0, progress: 11264/11916, step: 280, ave loss: 0.291265, ave acc: 0.906250, speed: 3.342159 steps/s\n",
            "2020-02-23 14:08:29,651-INFO: train pyreader queue size: 70, learning rate: 0.000015\n",
            "[INFO] 2020-02-23 14:08:29,651 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000015\n",
            "2020-02-23 14:08:29,652-INFO: epoch: 0, progress: 11584/11916, step: 290, ave loss: 0.331208, ave acc: 0.843750, speed: 3.344664 steps/s\n",
            "[INFO] 2020-02-23 14:08:29,652 [run_classifier.py:  287]:\tepoch: 0, progress: 11584/11916, step: 290, ave loss: 0.331208, ave acc: 0.843750, speed: 3.344664 steps/s\n",
            "2020-02-23 14:08:32,911-INFO: train pyreader queue size: 70, learning rate: 0.000015\n",
            "[INFO] 2020-02-23 14:08:32,911 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000015\n",
            "2020-02-23 14:08:32,911-INFO: epoch: 0, progress: 11904/11916, step: 300, ave loss: 0.352097, ave acc: 0.875000, speed: 3.068279 steps/s\n",
            "[INFO] 2020-02-23 14:08:32,911 [run_classifier.py:  287]:\tepoch: 0, progress: 11904/11916, step: 300, ave loss: 0.352097, ave acc: 0.875000, speed: 3.068279 steps/s\n",
            "2020-02-23 14:08:36,477-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:08:36,477 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:08:36,477-INFO: epoch: 1, progress: 288/11916, step: 310, ave loss: 0.567308, ave acc: 0.656250, speed: 2.803963 steps/s\n",
            "[INFO] 2020-02-23 14:08:36,477 [run_classifier.py:  287]:\tepoch: 1, progress: 288/11916, step: 310, ave loss: 0.567308, ave acc: 0.656250, speed: 2.803963 steps/s\n",
            "2020-02-23 14:08:36,481-INFO: validation result of dataset dataset/sem-eval/val.tsv:\n",
            "[INFO] 2020-02-23 14:08:36,481 [run_classifier.py:  390]:\tvalidation result of dataset dataset/sem-eval/val.tsv:\n",
            "2020-02-23 14:08:42,197-INFO: [dev evaluation] ave loss: 0.427339, acc:0.796073, data_num: 1324, elapsed time: 5.710029 s, file: dataset/sem-eval/val.tsv, epoch: 1, steps: 310\n",
            "[INFO] 2020-02-23 14:08:42,197 [run_classifier.py:  401]:\t[dev evaluation] ave loss: 0.427339, acc:0.796073, data_num: 1324, elapsed time: 5.710029 s, file: dataset/sem-eval/val.tsv, epoch: 1, steps: 310\n",
            "2020-02-23 14:08:42,205-INFO: testing dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.1.310\n",
            "[INFO] 2020-02-23 14:08:42,205 [run_classifier.py:  421]:\ttesting dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.1.310\n",
            "2020-02-23 14:08:48,730-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:08:48,730 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:08:48,730-INFO: epoch: 1, progress: 608/11916, step: 320, ave loss: 0.260835, ave acc: 0.875000, speed: 0.816151 steps/s\n",
            "[INFO] 2020-02-23 14:08:48,730 [run_classifier.py:  287]:\tepoch: 1, progress: 608/11916, step: 320, ave loss: 0.260835, ave acc: 0.875000, speed: 0.816151 steps/s\n",
            "2020-02-23 14:08:51,674-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:08:51,674 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:08:51,674-INFO: epoch: 1, progress: 928/11916, step: 330, ave loss: 0.299323, ave acc: 0.906250, speed: 3.397024 steps/s\n",
            "[INFO] 2020-02-23 14:08:51,674 [run_classifier.py:  287]:\tepoch: 1, progress: 928/11916, step: 330, ave loss: 0.299323, ave acc: 0.906250, speed: 3.397024 steps/s\n",
            "2020-02-23 14:08:54,609-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:08:54,609 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:08:54,609-INFO: epoch: 1, progress: 1248/11916, step: 340, ave loss: 0.353260, ave acc: 0.812500, speed: 3.407192 steps/s\n",
            "[INFO] 2020-02-23 14:08:54,609 [run_classifier.py:  287]:\tepoch: 1, progress: 1248/11916, step: 340, ave loss: 0.353260, ave acc: 0.812500, speed: 3.407192 steps/s\n",
            "2020-02-23 14:08:57,975-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:08:57,975 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:08:57,975-INFO: epoch: 1, progress: 1568/11916, step: 350, ave loss: 0.536817, ave acc: 0.718750, speed: 2.970678 steps/s\n",
            "[INFO] 2020-02-23 14:08:57,975 [run_classifier.py:  287]:\tepoch: 1, progress: 1568/11916, step: 350, ave loss: 0.536817, ave acc: 0.718750, speed: 2.970678 steps/s\n",
            "2020-02-23 14:09:00,724-INFO: train pyreader queue size: 70, learning rate: 0.000014\n",
            "[INFO] 2020-02-23 14:09:00,724 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000014\n",
            "2020-02-23 14:09:00,724-INFO: epoch: 1, progress: 1888/11916, step: 360, ave loss: 0.508713, ave acc: 0.812500, speed: 3.637843 steps/s\n",
            "[INFO] 2020-02-23 14:09:00,724 [run_classifier.py:  287]:\tepoch: 1, progress: 1888/11916, step: 360, ave loss: 0.508713, ave acc: 0.812500, speed: 3.637843 steps/s\n",
            "2020-02-23 14:09:03,926-INFO: train pyreader queue size: 70, learning rate: 0.000013\n",
            "[INFO] 2020-02-23 14:09:03,926 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000013\n",
            "2020-02-23 14:09:03,926-INFO: epoch: 1, progress: 2208/11916, step: 370, ave loss: 0.530416, ave acc: 0.750000, speed: 3.123662 steps/s\n",
            "[INFO] 2020-02-23 14:09:03,926 [run_classifier.py:  287]:\tepoch: 1, progress: 2208/11916, step: 370, ave loss: 0.530416, ave acc: 0.750000, speed: 3.123662 steps/s\n",
            "2020-02-23 14:09:07,057-INFO: train pyreader queue size: 70, learning rate: 0.000013\n",
            "[INFO] 2020-02-23 14:09:07,057 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000013\n",
            "2020-02-23 14:09:07,057-INFO: epoch: 1, progress: 2528/11916, step: 380, ave loss: 0.299702, ave acc: 0.875000, speed: 3.193649 steps/s\n",
            "[INFO] 2020-02-23 14:09:07,057 [run_classifier.py:  287]:\tepoch: 1, progress: 2528/11916, step: 380, ave loss: 0.299702, ave acc: 0.875000, speed: 3.193649 steps/s\n",
            "2020-02-23 14:09:10,127-INFO: train pyreader queue size: 70, learning rate: 0.000013\n",
            "[INFO] 2020-02-23 14:09:10,127 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000013\n",
            "2020-02-23 14:09:10,127-INFO: epoch: 1, progress: 2848/11916, step: 390, ave loss: 0.283422, ave acc: 0.843750, speed: 3.257450 steps/s\n",
            "[INFO] 2020-02-23 14:09:10,127 [run_classifier.py:  287]:\tepoch: 1, progress: 2848/11916, step: 390, ave loss: 0.283422, ave acc: 0.843750, speed: 3.257450 steps/s\n",
            "2020-02-23 14:09:13,020-INFO: train pyreader queue size: 70, learning rate: 0.000013\n",
            "[INFO] 2020-02-23 14:09:13,020 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000013\n",
            "2020-02-23 14:09:13,020-INFO: epoch: 1, progress: 3168/11916, step: 400, ave loss: 0.342080, ave acc: 0.906250, speed: 3.457293 steps/s\n",
            "[INFO] 2020-02-23 14:09:13,020 [run_classifier.py:  287]:\tepoch: 1, progress: 3168/11916, step: 400, ave loss: 0.342080, ave acc: 0.906250, speed: 3.457293 steps/s\n",
            "2020-02-23 14:09:15,941-INFO: train pyreader queue size: 70, learning rate: 0.000013\n",
            "[INFO] 2020-02-23 14:09:15,941 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000013\n",
            "2020-02-23 14:09:15,942-INFO: epoch: 1, progress: 3488/11916, step: 410, ave loss: 0.337069, ave acc: 0.812500, speed: 3.422818 steps/s\n",
            "[INFO] 2020-02-23 14:09:15,942 [run_classifier.py:  287]:\tepoch: 1, progress: 3488/11916, step: 410, ave loss: 0.337069, ave acc: 0.812500, speed: 3.422818 steps/s\n",
            "2020-02-23 14:09:19,239-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:19,239 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:19,239-INFO: epoch: 1, progress: 3808/11916, step: 420, ave loss: 0.140406, ave acc: 0.937500, speed: 3.032680 steps/s\n",
            "[INFO] 2020-02-23 14:09:19,239 [run_classifier.py:  287]:\tepoch: 1, progress: 3808/11916, step: 420, ave loss: 0.140406, ave acc: 0.937500, speed: 3.032680 steps/s\n",
            "2020-02-23 14:09:22,453-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:22,453 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:22,453-INFO: epoch: 1, progress: 4128/11916, step: 430, ave loss: 0.398272, ave acc: 0.812500, speed: 3.111525 steps/s\n",
            "[INFO] 2020-02-23 14:09:22,453 [run_classifier.py:  287]:\tepoch: 1, progress: 4128/11916, step: 430, ave loss: 0.398272, ave acc: 0.812500, speed: 3.111525 steps/s\n",
            "2020-02-23 14:09:25,802-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:25,802 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:25,802-INFO: epoch: 1, progress: 4448/11916, step: 440, ave loss: 0.404968, ave acc: 0.781250, speed: 2.985906 steps/s\n",
            "[INFO] 2020-02-23 14:09:25,802 [run_classifier.py:  287]:\tepoch: 1, progress: 4448/11916, step: 440, ave loss: 0.404968, ave acc: 0.781250, speed: 2.985906 steps/s\n",
            "2020-02-23 14:09:28,824-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:28,824 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:28,825-INFO: epoch: 1, progress: 4768/11916, step: 450, ave loss: 0.274737, ave acc: 0.875000, speed: 3.308735 steps/s\n",
            "[INFO] 2020-02-23 14:09:28,825 [run_classifier.py:  287]:\tepoch: 1, progress: 4768/11916, step: 450, ave loss: 0.274737, ave acc: 0.875000, speed: 3.308735 steps/s\n",
            "2020-02-23 14:09:32,230-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:32,230 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:32,231-INFO: epoch: 1, progress: 5088/11916, step: 460, ave loss: 0.354578, ave acc: 0.812500, speed: 2.936189 steps/s\n",
            "[INFO] 2020-02-23 14:09:32,231 [run_classifier.py:  287]:\tepoch: 1, progress: 5088/11916, step: 460, ave loss: 0.354578, ave acc: 0.812500, speed: 2.936189 steps/s\n",
            "2020-02-23 14:09:35,236-INFO: train pyreader queue size: 70, learning rate: 0.000012\n",
            "[INFO] 2020-02-23 14:09:35,236 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000012\n",
            "2020-02-23 14:09:35,236-INFO: epoch: 1, progress: 5408/11916, step: 470, ave loss: 0.368304, ave acc: 0.875000, speed: 3.327550 steps/s\n",
            "[INFO] 2020-02-23 14:09:35,236 [run_classifier.py:  287]:\tepoch: 1, progress: 5408/11916, step: 470, ave loss: 0.368304, ave acc: 0.875000, speed: 3.327550 steps/s\n",
            "2020-02-23 14:09:38,620-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:38,620 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:38,621-INFO: epoch: 1, progress: 5728/11916, step: 480, ave loss: 0.631254, ave acc: 0.718750, speed: 2.954685 steps/s\n",
            "[INFO] 2020-02-23 14:09:38,621 [run_classifier.py:  287]:\tepoch: 1, progress: 5728/11916, step: 480, ave loss: 0.631254, ave acc: 0.718750, speed: 2.954685 steps/s\n",
            "2020-02-23 14:09:41,713-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:41,713 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:41,713-INFO: epoch: 1, progress: 6048/11916, step: 490, ave loss: 0.499477, ave acc: 0.718750, speed: 3.233859 steps/s\n",
            "[INFO] 2020-02-23 14:09:41,713 [run_classifier.py:  287]:\tepoch: 1, progress: 6048/11916, step: 490, ave loss: 0.499477, ave acc: 0.718750, speed: 3.233859 steps/s\n",
            "2020-02-23 14:09:44,996-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:44,996 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:44,996-INFO: epoch: 1, progress: 6368/11916, step: 500, ave loss: 0.406036, ave acc: 0.843750, speed: 3.046046 steps/s\n",
            "[INFO] 2020-02-23 14:09:44,996 [run_classifier.py:  287]:\tepoch: 1, progress: 6368/11916, step: 500, ave loss: 0.406036, ave acc: 0.843750, speed: 3.046046 steps/s\n",
            "2020-02-23 14:09:48,226-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:48,226 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:48,226-INFO: epoch: 1, progress: 6688/11916, step: 510, ave loss: 0.313647, ave acc: 0.906250, speed: 3.095687 steps/s\n",
            "[INFO] 2020-02-23 14:09:48,226 [run_classifier.py:  287]:\tepoch: 1, progress: 6688/11916, step: 510, ave loss: 0.313647, ave acc: 0.906250, speed: 3.095687 steps/s\n",
            "2020-02-23 14:09:51,362-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:51,362 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:51,363-INFO: epoch: 1, progress: 7008/11916, step: 520, ave loss: 0.413665, ave acc: 0.843750, speed: 3.188686 steps/s\n",
            "[INFO] 2020-02-23 14:09:51,363 [run_classifier.py:  287]:\tepoch: 1, progress: 7008/11916, step: 520, ave loss: 0.413665, ave acc: 0.843750, speed: 3.188686 steps/s\n",
            "2020-02-23 14:09:54,633-INFO: train pyreader queue size: 70, learning rate: 0.000011\n",
            "[INFO] 2020-02-23 14:09:54,633 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000011\n",
            "2020-02-23 14:09:54,634-INFO: epoch: 1, progress: 7328/11916, step: 530, ave loss: 0.244321, ave acc: 0.906250, speed: 3.057363 steps/s\n",
            "[INFO] 2020-02-23 14:09:54,634 [run_classifier.py:  287]:\tepoch: 1, progress: 7328/11916, step: 530, ave loss: 0.244321, ave acc: 0.906250, speed: 3.057363 steps/s\n",
            "2020-02-23 14:09:58,266-INFO: train pyreader queue size: 70, learning rate: 0.000010\n",
            "[INFO] 2020-02-23 14:09:58,266 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000010\n",
            "2020-02-23 14:09:58,266-INFO: epoch: 1, progress: 7648/11916, step: 540, ave loss: 0.373918, ave acc: 0.875000, speed: 2.752896 steps/s\n",
            "[INFO] 2020-02-23 14:09:58,266 [run_classifier.py:  287]:\tepoch: 1, progress: 7648/11916, step: 540, ave loss: 0.373918, ave acc: 0.875000, speed: 2.752896 steps/s\n",
            "2020-02-23 14:10:01,154-INFO: train pyreader queue size: 70, learning rate: 0.000010\n",
            "[INFO] 2020-02-23 14:10:01,154 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000010\n",
            "2020-02-23 14:10:01,154-INFO: epoch: 1, progress: 7968/11916, step: 550, ave loss: 0.367677, ave acc: 0.843750, speed: 3.463075 steps/s\n",
            "[INFO] 2020-02-23 14:10:01,154 [run_classifier.py:  287]:\tepoch: 1, progress: 7968/11916, step: 550, ave loss: 0.367677, ave acc: 0.843750, speed: 3.463075 steps/s\n",
            "2020-02-23 14:10:04,032-INFO: train pyreader queue size: 70, learning rate: 0.000010\n",
            "[INFO] 2020-02-23 14:10:04,032 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000010\n",
            "2020-02-23 14:10:04,032-INFO: epoch: 1, progress: 8288/11916, step: 560, ave loss: 0.241099, ave acc: 0.906250, speed: 3.474775 steps/s\n",
            "[INFO] 2020-02-23 14:10:04,032 [run_classifier.py:  287]:\tepoch: 1, progress: 8288/11916, step: 560, ave loss: 0.241099, ave acc: 0.906250, speed: 3.474775 steps/s\n",
            "2020-02-23 14:10:06,951-INFO: train pyreader queue size: 70, learning rate: 0.000010\n",
            "[INFO] 2020-02-23 14:10:06,951 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000010\n",
            "2020-02-23 14:10:06,952-INFO: epoch: 1, progress: 8608/11916, step: 570, ave loss: 0.592348, ave acc: 0.718750, speed: 3.425518 steps/s\n",
            "[INFO] 2020-02-23 14:10:06,952 [run_classifier.py:  287]:\tepoch: 1, progress: 8608/11916, step: 570, ave loss: 0.592348, ave acc: 0.718750, speed: 3.425518 steps/s\n",
            "2020-02-23 14:10:09,798-INFO: train pyreader queue size: 70, learning rate: 0.000010\n",
            "[INFO] 2020-02-23 14:10:09,798 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000010\n",
            "2020-02-23 14:10:09,799-INFO: epoch: 1, progress: 8928/11916, step: 580, ave loss: 0.230746, ave acc: 0.937500, speed: 3.512449 steps/s\n",
            "[INFO] 2020-02-23 14:10:09,799 [run_classifier.py:  287]:\tepoch: 1, progress: 8928/11916, step: 580, ave loss: 0.230746, ave acc: 0.937500, speed: 3.512449 steps/s\n",
            "2020-02-23 14:10:12,909-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:12,909 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:12,909-INFO: epoch: 1, progress: 9248/11916, step: 590, ave loss: 0.430237, ave acc: 0.750000, speed: 3.214802 steps/s\n",
            "[INFO] 2020-02-23 14:10:12,909 [run_classifier.py:  287]:\tepoch: 1, progress: 9248/11916, step: 590, ave loss: 0.430237, ave acc: 0.750000, speed: 3.214802 steps/s\n",
            "2020-02-23 14:10:16,076-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:16,076 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:16,076-INFO: epoch: 1, progress: 9568/11916, step: 600, ave loss: 0.189679, ave acc: 0.968750, speed: 3.158269 steps/s\n",
            "[INFO] 2020-02-23 14:10:16,076 [run_classifier.py:  287]:\tepoch: 1, progress: 9568/11916, step: 600, ave loss: 0.189679, ave acc: 0.968750, speed: 3.158269 steps/s\n",
            "2020-02-23 14:10:19,341-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:19,341 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:19,341-INFO: epoch: 1, progress: 9888/11916, step: 610, ave loss: 0.264228, ave acc: 0.906250, speed: 3.062848 steps/s\n",
            "[INFO] 2020-02-23 14:10:19,341 [run_classifier.py:  287]:\tepoch: 1, progress: 9888/11916, step: 610, ave loss: 0.264228, ave acc: 0.906250, speed: 3.062848 steps/s\n",
            "2020-02-23 14:10:22,501-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:22,501 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:22,501-INFO: epoch: 1, progress: 10208/11916, step: 620, ave loss: 0.316879, ave acc: 0.906250, speed: 3.164380 steps/s\n",
            "[INFO] 2020-02-23 14:10:22,501 [run_classifier.py:  287]:\tepoch: 1, progress: 10208/11916, step: 620, ave loss: 0.316879, ave acc: 0.906250, speed: 3.164380 steps/s\n",
            "2020-02-23 14:10:25,464-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:25,464 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:25,464-INFO: epoch: 1, progress: 10528/11916, step: 630, ave loss: 0.353197, ave acc: 0.812500, speed: 3.375639 steps/s\n",
            "[INFO] 2020-02-23 14:10:25,464 [run_classifier.py:  287]:\tepoch: 1, progress: 10528/11916, step: 630, ave loss: 0.353197, ave acc: 0.812500, speed: 3.375639 steps/s\n",
            "2020-02-23 14:10:28,800-INFO: train pyreader queue size: 70, learning rate: 0.000009\n",
            "[INFO] 2020-02-23 14:10:28,800 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000009\n",
            "2020-02-23 14:10:28,801-INFO: epoch: 1, progress: 10848/11916, step: 640, ave loss: 0.398351, ave acc: 0.781250, speed: 2.996936 steps/s\n",
            "[INFO] 2020-02-23 14:10:28,801 [run_classifier.py:  287]:\tepoch: 1, progress: 10848/11916, step: 640, ave loss: 0.398351, ave acc: 0.781250, speed: 2.996936 steps/s\n",
            "2020-02-23 14:10:31,641-INFO: train pyreader queue size: 70, learning rate: 0.000008\n",
            "[INFO] 2020-02-23 14:10:31,641 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000008\n",
            "2020-02-23 14:10:31,641-INFO: epoch: 1, progress: 11168/11916, step: 650, ave loss: 0.325799, ave acc: 0.875000, speed: 3.520600 steps/s\n",
            "[INFO] 2020-02-23 14:10:31,641 [run_classifier.py:  287]:\tepoch: 1, progress: 11168/11916, step: 650, ave loss: 0.325799, ave acc: 0.875000, speed: 3.520600 steps/s\n",
            "2020-02-23 14:10:34,643-INFO: train pyreader queue size: 70, learning rate: 0.000008\n",
            "[INFO] 2020-02-23 14:10:34,643 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000008\n",
            "2020-02-23 14:10:34,643-INFO: epoch: 1, progress: 11488/11916, step: 660, ave loss: 0.330082, ave acc: 0.781250, speed: 3.331291 steps/s\n",
            "[INFO] 2020-02-23 14:10:34,643 [run_classifier.py:  287]:\tepoch: 1, progress: 11488/11916, step: 660, ave loss: 0.330082, ave acc: 0.781250, speed: 3.331291 steps/s\n",
            "2020-02-23 14:10:37,994-INFO: train pyreader queue size: 70, learning rate: 0.000008\n",
            "[INFO] 2020-02-23 14:10:37,994 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000008\n",
            "2020-02-23 14:10:37,994-INFO: epoch: 1, progress: 11808/11916, step: 670, ave loss: 0.341592, ave acc: 0.875000, speed: 2.984286 steps/s\n",
            "[INFO] 2020-02-23 14:10:37,994 [run_classifier.py:  287]:\tepoch: 1, progress: 11808/11916, step: 670, ave loss: 0.341592, ave acc: 0.875000, speed: 2.984286 steps/s\n",
            "2020-02-23 14:10:41,037-INFO: train pyreader queue size: 70, learning rate: 0.000008\n",
            "[INFO] 2020-02-23 14:10:41,037 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000008\n",
            "2020-02-23 14:10:41,037-INFO: epoch: 2, progress: 192/11916, step: 680, ave loss: 0.542929, ave acc: 0.718750, speed: 3.286815 steps/s\n",
            "[INFO] 2020-02-23 14:10:41,037 [run_classifier.py:  287]:\tepoch: 2, progress: 192/11916, step: 680, ave loss: 0.542929, ave acc: 0.718750, speed: 3.286815 steps/s\n",
            "2020-02-23 14:10:41,041-INFO: validation result of dataset dataset/sem-eval/val.tsv:\n",
            "[INFO] 2020-02-23 14:10:41,041 [run_classifier.py:  390]:\tvalidation result of dataset dataset/sem-eval/val.tsv:\n",
            "2020-02-23 14:10:46,945-INFO: [dev evaluation] ave loss: 0.446337, acc:0.803625, data_num: 1324, elapsed time: 5.898463 s, file: dataset/sem-eval/val.tsv, epoch: 2, steps: 680\n",
            "[INFO] 2020-02-23 14:10:46,945 [run_classifier.py:  401]:\t[dev evaluation] ave loss: 0.446337, acc:0.803625, data_num: 1324, elapsed time: 5.898463 s, file: dataset/sem-eval/val.tsv, epoch: 2, steps: 680\n",
            "2020-02-23 14:10:46,948-INFO: testing dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.2.680\n",
            "[INFO] 2020-02-23 14:10:46,948 [run_classifier.py:  421]:\ttesting dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.2.680\n",
            "2020-02-23 14:10:54,010-INFO: train pyreader queue size: 70, learning rate: 0.000008\n",
            "[INFO] 2020-02-23 14:10:54,010 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000008\n",
            "2020-02-23 14:10:54,011-INFO: epoch: 2, progress: 512/11916, step: 690, ave loss: 0.468247, ave acc: 0.781250, speed: 0.770789 steps/s\n",
            "[INFO] 2020-02-23 14:10:54,011 [run_classifier.py:  287]:\tepoch: 2, progress: 512/11916, step: 690, ave loss: 0.468247, ave acc: 0.781250, speed: 0.770789 steps/s\n",
            "2020-02-23 14:10:57,426-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:10:57,426 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:10:57,427-INFO: epoch: 2, progress: 832/11916, step: 700, ave loss: 0.488865, ave acc: 0.781250, speed: 2.927745 steps/s\n",
            "[INFO] 2020-02-23 14:10:57,427 [run_classifier.py:  287]:\tepoch: 2, progress: 832/11916, step: 700, ave loss: 0.488865, ave acc: 0.781250, speed: 2.927745 steps/s\n",
            "2020-02-23 14:11:00,252-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:11:00,252 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:11:00,252-INFO: epoch: 2, progress: 1152/11916, step: 710, ave loss: 0.414581, ave acc: 0.843750, speed: 3.539035 steps/s\n",
            "[INFO] 2020-02-23 14:11:00,252 [run_classifier.py:  287]:\tepoch: 2, progress: 1152/11916, step: 710, ave loss: 0.414581, ave acc: 0.843750, speed: 3.539035 steps/s\n",
            "2020-02-23 14:11:03,189-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:11:03,189 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:11:03,189-INFO: epoch: 2, progress: 1472/11916, step: 720, ave loss: 0.533223, ave acc: 0.781250, speed: 3.405282 steps/s\n",
            "[INFO] 2020-02-23 14:11:03,189 [run_classifier.py:  287]:\tepoch: 2, progress: 1472/11916, step: 720, ave loss: 0.533223, ave acc: 0.781250, speed: 3.405282 steps/s\n",
            "2020-02-23 14:11:06,726-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:11:06,726 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:11:06,726-INFO: epoch: 2, progress: 1792/11916, step: 730, ave loss: 0.490355, ave acc: 0.812500, speed: 2.827242 steps/s\n",
            "[INFO] 2020-02-23 14:11:06,726 [run_classifier.py:  287]:\tepoch: 2, progress: 1792/11916, step: 730, ave loss: 0.490355, ave acc: 0.812500, speed: 2.827242 steps/s\n",
            "2020-02-23 14:11:09,945-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:11:09,945 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:11:09,945-INFO: epoch: 2, progress: 2112/11916, step: 740, ave loss: 0.226161, ave acc: 0.875000, speed: 3.106384 steps/s\n",
            "[INFO] 2020-02-23 14:11:09,945 [run_classifier.py:  287]:\tepoch: 2, progress: 2112/11916, step: 740, ave loss: 0.226161, ave acc: 0.875000, speed: 3.106384 steps/s\n",
            "2020-02-23 14:11:12,516-INFO: train pyreader queue size: 70, learning rate: 0.000007\n",
            "[INFO] 2020-02-23 14:11:12,516 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000007\n",
            "2020-02-23 14:11:12,516-INFO: epoch: 2, progress: 2432/11916, step: 750, ave loss: 0.289307, ave acc: 0.781250, speed: 3.890688 steps/s\n",
            "[INFO] 2020-02-23 14:11:12,516 [run_classifier.py:  287]:\tepoch: 2, progress: 2432/11916, step: 750, ave loss: 0.289307, ave acc: 0.781250, speed: 3.890688 steps/s\n",
            "2020-02-23 14:11:15,862-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:15,862 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:15,862-INFO: epoch: 2, progress: 2752/11916, step: 760, ave loss: 0.286288, ave acc: 0.937500, speed: 2.988645 steps/s\n",
            "[INFO] 2020-02-23 14:11:15,862 [run_classifier.py:  287]:\tepoch: 2, progress: 2752/11916, step: 760, ave loss: 0.286288, ave acc: 0.937500, speed: 2.988645 steps/s\n",
            "2020-02-23 14:11:19,004-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:19,004 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:19,005-INFO: epoch: 2, progress: 3072/11916, step: 770, ave loss: 0.380753, ave acc: 0.781250, speed: 3.182058 steps/s\n",
            "[INFO] 2020-02-23 14:11:19,005 [run_classifier.py:  287]:\tepoch: 2, progress: 3072/11916, step: 770, ave loss: 0.380753, ave acc: 0.781250, speed: 3.182058 steps/s\n",
            "2020-02-23 14:11:22,576-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:22,576 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:22,576-INFO: epoch: 2, progress: 3392/11916, step: 780, ave loss: 0.208796, ave acc: 0.906250, speed: 2.800181 steps/s\n",
            "[INFO] 2020-02-23 14:11:22,576 [run_classifier.py:  287]:\tepoch: 2, progress: 3392/11916, step: 780, ave loss: 0.208796, ave acc: 0.906250, speed: 2.800181 steps/s\n",
            "2020-02-23 14:11:25,549-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:25,549 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:25,549-INFO: epoch: 2, progress: 3712/11916, step: 790, ave loss: 0.469651, ave acc: 0.781250, speed: 3.363867 steps/s\n",
            "[INFO] 2020-02-23 14:11:25,549 [run_classifier.py:  287]:\tepoch: 2, progress: 3712/11916, step: 790, ave loss: 0.469651, ave acc: 0.781250, speed: 3.363867 steps/s\n",
            "2020-02-23 14:11:28,720-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:28,720 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:28,720-INFO: epoch: 2, progress: 4032/11916, step: 800, ave loss: 0.315401, ave acc: 0.875000, speed: 3.153400 steps/s\n",
            "[INFO] 2020-02-23 14:11:28,720 [run_classifier.py:  287]:\tepoch: 2, progress: 4032/11916, step: 800, ave loss: 0.315401, ave acc: 0.875000, speed: 3.153400 steps/s\n",
            "2020-02-23 14:11:32,014-INFO: train pyreader queue size: 70, learning rate: 0.000006\n",
            "[INFO] 2020-02-23 14:11:32,014 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000006\n",
            "2020-02-23 14:11:32,014-INFO: epoch: 2, progress: 4352/11916, step: 810, ave loss: 0.289702, ave acc: 0.843750, speed: 3.036075 steps/s\n",
            "[INFO] 2020-02-23 14:11:32,014 [run_classifier.py:  287]:\tepoch: 2, progress: 4352/11916, step: 810, ave loss: 0.289702, ave acc: 0.843750, speed: 3.036075 steps/s\n",
            "2020-02-23 14:11:35,111-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:11:35,111 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:11:35,111-INFO: epoch: 2, progress: 4672/11916, step: 820, ave loss: 0.499724, ave acc: 0.812500, speed: 3.229040 steps/s\n",
            "[INFO] 2020-02-23 14:11:35,111 [run_classifier.py:  287]:\tepoch: 2, progress: 4672/11916, step: 820, ave loss: 0.499724, ave acc: 0.812500, speed: 3.229040 steps/s\n",
            "2020-02-23 14:11:38,324-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:11:38,324 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:11:38,325-INFO: epoch: 2, progress: 4992/11916, step: 830, ave loss: 0.123535, ave acc: 1.000000, speed: 3.111962 steps/s\n",
            "[INFO] 2020-02-23 14:11:38,325 [run_classifier.py:  287]:\tepoch: 2, progress: 4992/11916, step: 830, ave loss: 0.123535, ave acc: 1.000000, speed: 3.111962 steps/s\n",
            "2020-02-23 14:11:41,276-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:11:41,276 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:11:41,276-INFO: epoch: 2, progress: 5312/11916, step: 840, ave loss: 0.157091, ave acc: 0.937500, speed: 3.388172 steps/s\n",
            "[INFO] 2020-02-23 14:11:41,276 [run_classifier.py:  287]:\tepoch: 2, progress: 5312/11916, step: 840, ave loss: 0.157091, ave acc: 0.937500, speed: 3.388172 steps/s\n",
            "2020-02-23 14:11:44,508-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:11:44,508 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:11:44,508-INFO: epoch: 2, progress: 5632/11916, step: 850, ave loss: 0.236989, ave acc: 0.906250, speed: 3.094378 steps/s\n",
            "[INFO] 2020-02-23 14:11:44,508 [run_classifier.py:  287]:\tepoch: 2, progress: 5632/11916, step: 850, ave loss: 0.236989, ave acc: 0.906250, speed: 3.094378 steps/s\n",
            "2020-02-23 14:11:47,950-INFO: train pyreader queue size: 70, learning rate: 0.000005\n",
            "[INFO] 2020-02-23 14:11:47,950 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000005\n",
            "2020-02-23 14:11:47,950-INFO: epoch: 2, progress: 5952/11916, step: 860, ave loss: 0.334301, ave acc: 0.843750, speed: 2.905175 steps/s\n",
            "[INFO] 2020-02-23 14:11:47,950 [run_classifier.py:  287]:\tepoch: 2, progress: 5952/11916, step: 860, ave loss: 0.334301, ave acc: 0.843750, speed: 2.905175 steps/s\n",
            "2020-02-23 14:11:50,978-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:11:50,978 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:11:50,978-INFO: epoch: 2, progress: 6272/11916, step: 870, ave loss: 0.285169, ave acc: 0.906250, speed: 3.302797 steps/s\n",
            "[INFO] 2020-02-23 14:11:50,978 [run_classifier.py:  287]:\tepoch: 2, progress: 6272/11916, step: 870, ave loss: 0.285169, ave acc: 0.906250, speed: 3.302797 steps/s\n",
            "2020-02-23 14:11:54,172-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:11:54,172 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:11:54,172-INFO: epoch: 2, progress: 6592/11916, step: 880, ave loss: 0.252666, ave acc: 0.875000, speed: 3.131151 steps/s\n",
            "[INFO] 2020-02-23 14:11:54,172 [run_classifier.py:  287]:\tepoch: 2, progress: 6592/11916, step: 880, ave loss: 0.252666, ave acc: 0.875000, speed: 3.131151 steps/s\n",
            "2020-02-23 14:11:57,272-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:11:57,272 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:11:57,272-INFO: epoch: 2, progress: 6912/11916, step: 890, ave loss: 0.406427, ave acc: 0.875000, speed: 3.225753 steps/s\n",
            "[INFO] 2020-02-23 14:11:57,272 [run_classifier.py:  287]:\tepoch: 2, progress: 6912/11916, step: 890, ave loss: 0.406427, ave acc: 0.875000, speed: 3.225753 steps/s\n",
            "2020-02-23 14:12:00,345-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:12:00,345 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:12:00,345-INFO: epoch: 2, progress: 7232/11916, step: 900, ave loss: 0.232964, ave acc: 0.875000, speed: 3.254704 steps/s\n",
            "[INFO] 2020-02-23 14:12:00,345 [run_classifier.py:  287]:\tepoch: 2, progress: 7232/11916, step: 900, ave loss: 0.232964, ave acc: 0.875000, speed: 3.254704 steps/s\n",
            "2020-02-23 14:12:03,333-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:12:03,333 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:12:03,333-INFO: epoch: 2, progress: 7552/11916, step: 910, ave loss: 0.367090, ave acc: 0.875000, speed: 3.346968 steps/s\n",
            "[INFO] 2020-02-23 14:12:03,333 [run_classifier.py:  287]:\tepoch: 2, progress: 7552/11916, step: 910, ave loss: 0.367090, ave acc: 0.875000, speed: 3.346968 steps/s\n",
            "2020-02-23 14:12:06,438-INFO: train pyreader queue size: 70, learning rate: 0.000004\n",
            "[INFO] 2020-02-23 14:12:06,438 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000004\n",
            "2020-02-23 14:12:06,438-INFO: epoch: 2, progress: 7872/11916, step: 920, ave loss: 0.260700, ave acc: 0.875000, speed: 3.220293 steps/s\n",
            "[INFO] 2020-02-23 14:12:06,438 [run_classifier.py:  287]:\tepoch: 2, progress: 7872/11916, step: 920, ave loss: 0.260700, ave acc: 0.875000, speed: 3.220293 steps/s\n",
            "2020-02-23 14:12:09,778-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:12:09,778 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:12:09,778-INFO: epoch: 2, progress: 8192/11916, step: 930, ave loss: 0.237013, ave acc: 0.906250, speed: 2.994661 steps/s\n",
            "[INFO] 2020-02-23 14:12:09,778 [run_classifier.py:  287]:\tepoch: 2, progress: 8192/11916, step: 930, ave loss: 0.237013, ave acc: 0.906250, speed: 2.994661 steps/s\n",
            "2020-02-23 14:12:12,741-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:12:12,741 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:12:12,741-INFO: epoch: 2, progress: 8512/11916, step: 940, ave loss: 0.280523, ave acc: 0.937500, speed: 3.374726 steps/s\n",
            "[INFO] 2020-02-23 14:12:12,741 [run_classifier.py:  287]:\tepoch: 2, progress: 8512/11916, step: 940, ave loss: 0.280523, ave acc: 0.937500, speed: 3.374726 steps/s\n",
            "2020-02-23 14:12:15,461-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:12:15,461 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:12:15,461-INFO: epoch: 2, progress: 8832/11916, step: 950, ave loss: 0.256235, ave acc: 0.906250, speed: 3.677081 steps/s\n",
            "[INFO] 2020-02-23 14:12:15,461 [run_classifier.py:  287]:\tepoch: 2, progress: 8832/11916, step: 950, ave loss: 0.256235, ave acc: 0.906250, speed: 3.677081 steps/s\n",
            "2020-02-23 14:12:18,516-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:12:18,516 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:12:18,517-INFO: epoch: 2, progress: 9152/11916, step: 960, ave loss: 0.310877, ave acc: 0.875000, speed: 3.272486 steps/s\n",
            "[INFO] 2020-02-23 14:12:18,517 [run_classifier.py:  287]:\tepoch: 2, progress: 9152/11916, step: 960, ave loss: 0.310877, ave acc: 0.875000, speed: 3.272486 steps/s\n",
            "2020-02-23 14:12:21,644-INFO: train pyreader queue size: 70, learning rate: 0.000003\n",
            "[INFO] 2020-02-23 14:12:21,644 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000003\n",
            "2020-02-23 14:12:21,644-INFO: epoch: 2, progress: 9472/11916, step: 970, ave loss: 0.190999, ave acc: 0.968750, speed: 3.197752 steps/s\n",
            "[INFO] 2020-02-23 14:12:21,644 [run_classifier.py:  287]:\tepoch: 2, progress: 9472/11916, step: 970, ave loss: 0.190999, ave acc: 0.968750, speed: 3.197752 steps/s\n",
            "2020-02-23 14:12:24,816-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:24,816 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:24,817-INFO: epoch: 2, progress: 9792/11916, step: 980, ave loss: 0.309598, ave acc: 0.843750, speed: 3.152070 steps/s\n",
            "[INFO] 2020-02-23 14:12:24,817 [run_classifier.py:  287]:\tepoch: 2, progress: 9792/11916, step: 980, ave loss: 0.309598, ave acc: 0.843750, speed: 3.152070 steps/s\n",
            "2020-02-23 14:12:28,355-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:28,355 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:28,355-INFO: epoch: 2, progress: 10112/11916, step: 990, ave loss: 0.398910, ave acc: 0.812500, speed: 2.826251 steps/s\n",
            "[INFO] 2020-02-23 14:12:28,355 [run_classifier.py:  287]:\tepoch: 2, progress: 10112/11916, step: 990, ave loss: 0.398910, ave acc: 0.812500, speed: 2.826251 steps/s\n",
            "2020-02-23 14:12:31,353-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:31,353 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:31,353-INFO: epoch: 2, progress: 10432/11916, step: 1000, ave loss: 0.429365, ave acc: 0.843750, speed: 3.336058 steps/s\n",
            "[INFO] 2020-02-23 14:12:31,353 [run_classifier.py:  287]:\tepoch: 2, progress: 10432/11916, step: 1000, ave loss: 0.429365, ave acc: 0.843750, speed: 3.336058 steps/s\n",
            "2020-02-23 14:12:34,562-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:34,562 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:34,563-INFO: epoch: 2, progress: 10752/11916, step: 1010, ave loss: 0.297767, ave acc: 0.906250, speed: 3.115624 steps/s\n",
            "[INFO] 2020-02-23 14:12:34,563 [run_classifier.py:  287]:\tepoch: 2, progress: 10752/11916, step: 1010, ave loss: 0.297767, ave acc: 0.906250, speed: 3.115624 steps/s\n",
            "2020-02-23 14:12:37,556-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:37,556 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:37,556-INFO: epoch: 2, progress: 11072/11916, step: 1020, ave loss: 0.150132, ave acc: 0.937500, speed: 3.340498 steps/s\n",
            "[INFO] 2020-02-23 14:12:37,556 [run_classifier.py:  287]:\tepoch: 2, progress: 11072/11916, step: 1020, ave loss: 0.150132, ave acc: 0.937500, speed: 3.340498 steps/s\n",
            "2020-02-23 14:12:40,555-INFO: train pyreader queue size: 70, learning rate: 0.000002\n",
            "[INFO] 2020-02-23 14:12:40,555 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000002\n",
            "2020-02-23 14:12:40,555-INFO: epoch: 2, progress: 11392/11916, step: 1030, ave loss: 0.203429, ave acc: 0.968750, speed: 3.334606 steps/s\n",
            "[INFO] 2020-02-23 14:12:40,555 [run_classifier.py:  287]:\tepoch: 2, progress: 11392/11916, step: 1030, ave loss: 0.203429, ave acc: 0.968750, speed: 3.334606 steps/s\n",
            "2020-02-23 14:12:43,620-INFO: train pyreader queue size: 70, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:43,620 [run_classifier.py:  275]:\ttrain pyreader queue size: 70, learning rate: 0.000001\n",
            "2020-02-23 14:12:43,620-INFO: epoch: 2, progress: 11712/11916, step: 1040, ave loss: 0.238528, ave acc: 0.937500, speed: 3.262947 steps/s\n",
            "[INFO] 2020-02-23 14:12:43,620 [run_classifier.py:  287]:\tepoch: 2, progress: 11712/11916, step: 1040, ave loss: 0.238528, ave acc: 0.937500, speed: 3.262947 steps/s\n",
            "2020-02-23 14:12:46,605-INFO: train pyreader queue size: 68, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:46,605 [run_classifier.py:  275]:\ttrain pyreader queue size: 68, learning rate: 0.000001\n",
            "2020-02-23 14:12:46,605-INFO: epoch: 2, progress: 11915/11916, step: 1050, ave loss: 0.396360, ave acc: 0.812500, speed: 3.350279 steps/s\n",
            "[INFO] 2020-02-23 14:12:46,605 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1050, ave loss: 0.396360, ave acc: 0.812500, speed: 3.350279 steps/s\n",
            "2020-02-23 14:12:49,951-INFO: train pyreader queue size: 58, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:49,951 [run_classifier.py:  275]:\ttrain pyreader queue size: 58, learning rate: 0.000001\n",
            "2020-02-23 14:12:49,952-INFO: epoch: 2, progress: 11915/11916, step: 1060, ave loss: 0.243113, ave acc: 0.906250, speed: 2.988357 steps/s\n",
            "[INFO] 2020-02-23 14:12:49,952 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1060, ave loss: 0.243113, ave acc: 0.906250, speed: 2.988357 steps/s\n",
            "2020-02-23 14:12:53,007-INFO: train pyreader queue size: 48, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:53,007 [run_classifier.py:  275]:\ttrain pyreader queue size: 48, learning rate: 0.000001\n",
            "2020-02-23 14:12:53,007-INFO: epoch: 2, progress: 11915/11916, step: 1070, ave loss: 0.320804, ave acc: 0.812500, speed: 3.273079 steps/s\n",
            "[INFO] 2020-02-23 14:12:53,007 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1070, ave loss: 0.320804, ave acc: 0.812500, speed: 3.273079 steps/s\n",
            "2020-02-23 14:12:55,956-INFO: train pyreader queue size: 38, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:55,956 [run_classifier.py:  275]:\ttrain pyreader queue size: 38, learning rate: 0.000001\n",
            "2020-02-23 14:12:55,957-INFO: epoch: 2, progress: 11915/11916, step: 1080, ave loss: 0.183242, ave acc: 0.937500, speed: 3.390209 steps/s\n",
            "[INFO] 2020-02-23 14:12:55,957 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1080, ave loss: 0.183242, ave acc: 0.937500, speed: 3.390209 steps/s\n",
            "2020-02-23 14:12:58,909-INFO: train pyreader queue size: 28, learning rate: 0.000001\n",
            "[INFO] 2020-02-23 14:12:58,909 [run_classifier.py:  275]:\ttrain pyreader queue size: 28, learning rate: 0.000001\n",
            "2020-02-23 14:12:58,909-INFO: epoch: 2, progress: 11915/11916, step: 1090, ave loss: 0.305861, ave acc: 0.906250, speed: 3.387589 steps/s\n",
            "[INFO] 2020-02-23 14:12:58,909 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1090, ave loss: 0.305861, ave acc: 0.906250, speed: 3.387589 steps/s\n",
            "2020-02-23 14:13:02,000-INFO: train pyreader queue size: 18, learning rate: 0.000000\n",
            "[INFO] 2020-02-23 14:13:02,000 [run_classifier.py:  275]:\ttrain pyreader queue size: 18, learning rate: 0.000000\n",
            "2020-02-23 14:13:02,000-INFO: epoch: 2, progress: 11915/11916, step: 1100, ave loss: 0.337757, ave acc: 0.812500, speed: 3.234821 steps/s\n",
            "[INFO] 2020-02-23 14:13:02,000 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1100, ave loss: 0.337757, ave acc: 0.812500, speed: 3.234821 steps/s\n",
            "2020-02-23 14:13:05,114-INFO: train pyreader queue size: 8, learning rate: 0.000000\n",
            "[INFO] 2020-02-23 14:13:05,114 [run_classifier.py:  275]:\ttrain pyreader queue size: 8, learning rate: 0.000000\n",
            "2020-02-23 14:13:05,115-INFO: epoch: 2, progress: 11915/11916, step: 1110, ave loss: 0.426948, ave acc: 0.812500, speed: 3.211214 steps/s\n",
            "[INFO] 2020-02-23 14:13:05,115 [run_classifier.py:  287]:\tepoch: 2, progress: 11915/11916, step: 1110, ave loss: 0.426948, ave acc: 0.812500, speed: 3.211214 steps/s\n",
            "2020-02-23 14:13:12,344-INFO: validation result of dataset dataset/sem-eval/val.tsv:\n",
            "[INFO] 2020-02-23 14:13:12,344 [run_classifier.py:  390]:\tvalidation result of dataset dataset/sem-eval/val.tsv:\n",
            "2020-02-23 14:13:18,255-INFO: [dev evaluation] ave loss: 0.487989, acc:0.803625, data_num: 1324, elapsed time: 5.903244 s, file: dataset/sem-eval/val.tsv, epoch: 2, steps: 1120\n",
            "[INFO] 2020-02-23 14:13:18,255 [run_classifier.py:  401]:\t[dev evaluation] ave loss: 0.487989, acc:0.803625, data_num: 1324, elapsed time: 5.903244 s, file: dataset/sem-eval/val.tsv, epoch: 2, steps: 1120\n",
            "2020-02-23 14:13:18,257-INFO: testing dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.2.1120\n",
            "[INFO] 2020-02-23 14:13:18,257 [run_classifier.py:  421]:\ttesting dataset/sem-eval/test.tsv, save to output/test_out.{1..5}.2e-5.32.3.tsv.2.1120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyFpSJSk3CbK",
        "colab_type": "text"
      },
      "source": [
        "# **Processing output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikh7W7nnl-sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save file\n",
        "from google.colab import files\n",
        "file_name = \"output/test_out.{1..5}.2e-5.32.3.tsv.2.1120\"\n",
        "files.download(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZdhWvkCgk1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_out = pd.read_csv('gdrive/My Drive/Pracownia/Ernie/test_out.{1..5}.2e-5.32.4.tsv.3.1493', delimiter='\\t', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJgE_vI_nJ73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_out = pd.read_csv(file_name, delimiter='\\t', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU_aQGA4pQRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = re.compile(r'(,){2,}')\n",
        "\n",
        "def parse_element(el):\n",
        "  multiple_commas = el.replace(' ', ',')[1:-1]\n",
        "  one_comma = re.sub(pattern, ',', multiple_commas)\n",
        "  return float(one_comma.split(',')[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYIIFETDnis5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = [parse_element(el) for el in test_out[1]]\n",
        "float_test_y = [float(el) for el in test_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XzVUD15tOYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "319256b1-d30c-4bbb-8040-22f9077b05d7"
      },
      "source": [
        "from keras.metrics import binary_accuracy\n",
        "acc = binary_accuracy(float_test_y, preds)\n",
        "with tf.Session() as sess:\n",
        "  print(acc.eval()) "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.85232556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-0DBiYu7eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN76HnNVu9DD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eadf4eb-c608-4ecd-c676-aba622834296"
      },
      "source": [
        "f1_test = f1(tf.cast(test_y, tf.float32), preds)\n",
        "with tf.Session() as sess:\n",
        "  print(f1_test.eval()) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7196467\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}