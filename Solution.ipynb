{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c72-N0RGo1z",
        "colab_type": "code",
        "outputId": "2912d4a8-1aae-4bb0-ea7b-720ad6fea769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZvrBG6DXhkA",
        "colab_type": "code",
        "outputId": "d392f2a9-eb98-4945-98ba-edb2d0639fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd gdrive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "a1747bd6-bb22-4043-8da4-4ec94f45f940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.5)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.5)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.5)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "outputId": "30c6cd72-454c-40e2-b109-4975e9fa9f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf2\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "#import tensorflow_addons as tfa\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "ca80450c-d14f-4fec-bf3e-ca8254df1882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "'''\n",
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "testing_examples_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levela.tsv'\n",
        "testing_labels_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levela.csv'\n",
        "testing_examples_C_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levelc.tsv'\n",
        "testing_labels_C_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levelc.csv'\n",
        "\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "testing_dataset_examples_A = pd.read_csv(testing_examples_A_url, delimiter='\\t')\n",
        "testing_dataset_labels_A = pd.read_csv(testing_labels_A_url, delimiter=',')\n",
        "testing_dataset_examples_C = pd.read_csv(testing_examples_A_url, delimiter='\\t')\n",
        "testing_dataset_labels_C = pd.read_csv(testing_labels_A_url, delimiter=',')\n",
        "\n",
        "print(training_dataset.head())\n",
        "'''"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJNUAx_OiYP7",
        "colab_type": "text"
      },
      "source": [
        "## Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4FoVUy0f86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "17d1408e-f6b0-4e8f-b7ef-d1c8af69a6a3"
      },
      "source": [
        "'''\n",
        "#Import 2020 data\n",
        "\n",
        "training_dataset2020_A = pd.read_csv('Colab Notebooks/task_a_distant.tsv', delimiter='\\t', nrows=500000)\n",
        "training_dataset2020_C = pd.read_csv('Colab Notebooks/task_c_distant_ann.tsv', delimiter='\\t')\n",
        "training_dataset2020_C\n",
        "'''"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>average_ind</th>\n",
              "      <th>average_grp</th>\n",
              "      <th>average_oth</th>\n",
              "      <th>std_ind</th>\n",
              "      <th>std_grp</th>\n",
              "      <th>std_oth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1159533712079503361</td>\n",
              "      <td>@USER Trump is a fucking idiot his dementia is...</td>\n",
              "      <td>0.833432</td>\n",
              "      <td>0.076110</td>\n",
              "      <td>0.107765</td>\n",
              "      <td>0.208334</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.138649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1159533713044234241</td>\n",
              "      <td>@USER HELL YES! His grinned and thumbs up are ...</td>\n",
              "      <td>0.481062</td>\n",
              "      <td>0.367363</td>\n",
              "      <td>0.138841</td>\n",
              "      <td>0.345225</td>\n",
              "      <td>0.335924</td>\n",
              "      <td>0.083230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1159533718345830400</td>\n",
              "      <td>@USER Can't wait to see the shit show his deat...</td>\n",
              "      <td>0.438813</td>\n",
              "      <td>0.268574</td>\n",
              "      <td>0.377573</td>\n",
              "      <td>0.182609</td>\n",
              "      <td>0.186880</td>\n",
              "      <td>0.254621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1159533739871002625</td>\n",
              "      <td>@USER @USER @USER This guys is dumb check his ...</td>\n",
              "      <td>0.712995</td>\n",
              "      <td>0.123504</td>\n",
              "      <td>0.111130</td>\n",
              "      <td>0.248839</td>\n",
              "      <td>0.107572</td>\n",
              "      <td>0.067552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1159533742366633984</td>\n",
              "      <td>@USER @USER Fuck him better than his hoes</td>\n",
              "      <td>0.691414</td>\n",
              "      <td>0.146723</td>\n",
              "      <td>0.192282</td>\n",
              "      <td>0.204415</td>\n",
              "      <td>0.154818</td>\n",
              "      <td>0.104436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188968</th>\n",
              "      <td>1187532615030587394</td>\n",
              "      <td>If you lying bout lil shit, you lying bout a l...</td>\n",
              "      <td>0.690031</td>\n",
              "      <td>0.178021</td>\n",
              "      <td>0.201831</td>\n",
              "      <td>0.249506</td>\n",
              "      <td>0.189998</td>\n",
              "      <td>0.166256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188969</th>\n",
              "      <td>1187611970624610304</td>\n",
              "      <td>do yâ€™all ever like wanna grab somebodys face a...</td>\n",
              "      <td>0.503566</td>\n",
              "      <td>0.316713</td>\n",
              "      <td>0.260821</td>\n",
              "      <td>0.302630</td>\n",
              "      <td>0.146222</td>\n",
              "      <td>0.220385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188970</th>\n",
              "      <td>1187636295306268673</td>\n",
              "      <td>I meant tool but the point is women ainâ€™t shit...</td>\n",
              "      <td>0.363758</td>\n",
              "      <td>0.431517</td>\n",
              "      <td>0.282849</td>\n",
              "      <td>0.239436</td>\n",
              "      <td>0.165351</td>\n",
              "      <td>0.095065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188971</th>\n",
              "      <td>1187636299785744384</td>\n",
              "      <td>@USER i sang too the fuck-</td>\n",
              "      <td>0.598230</td>\n",
              "      <td>0.161560</td>\n",
              "      <td>0.251305</td>\n",
              "      <td>0.191871</td>\n",
              "      <td>0.107075</td>\n",
              "      <td>0.115206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188972</th>\n",
              "      <td>1187636316487520258</td>\n",
              "      <td>obviously ur allowed to not like the ship but ...</td>\n",
              "      <td>0.684948</td>\n",
              "      <td>0.152272</td>\n",
              "      <td>0.242591</td>\n",
              "      <td>0.192277</td>\n",
              "      <td>0.140813</td>\n",
              "      <td>0.165592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188973 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  ...   std_oth\n",
              "0       1159533712079503361  ...  0.138649\n",
              "1       1159533713044234241  ...  0.083230\n",
              "2       1159533718345830400  ...  0.254621\n",
              "3       1159533739871002625  ...  0.067552\n",
              "4       1159533742366633984  ...  0.104436\n",
              "...                     ...  ...       ...\n",
              "188968  1187532615030587394  ...  0.166256\n",
              "188969  1187611970624610304  ...  0.220385\n",
              "188970  1187636295306268673  ...  0.095065\n",
              "188971  1187636299785744384  ...  0.115206\n",
              "188972  1187636316487520258  ...  0.165592\n",
              "\n",
              "[188973 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LJzCAAo0fvP",
        "colab_type": "code",
        "outputId": "1a17d7a0-bcc7-496b-d1cb-f38fdf3b2c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "'''\n",
        "c_dict = ['IND', 'GRP', 'OTH']\n",
        "\n",
        "def convert_averages(row):\n",
        "    return 'OFF' if row['average'] >= 0.4 else 'NOT'\n",
        "\n",
        "def convert_averages_c(row):\n",
        "    averages = [row['average_ind'], row['average_grp'], row['average_oth']]\n",
        "    stds = [row['std_ind'], row['std_grp'], row['std_oth']]\n",
        "    max_avg_id = np.argmax(averages)\n",
        "    for i in range(3):\n",
        "        if i == max_avg_id:\n",
        "            continue\n",
        "\n",
        "        return c_dict[i] if max(0, averages[i] - stds[i]) - 0.3 > max(0, averages[max_avg_id] - stds[max_avg_id]) else c_dict[max_avg_id]\n",
        "\n",
        "\n",
        "training_dataset2020_A = training_dataset2020_A.drop(labels=['id', 'std'], axis=1)\n",
        "training_dataset2020_A['average'] = training_dataset2020_A.apply(lambda row: convert_averages(row), axis=1)\n",
        "training_dataset2020_A = training_dataset2020_A.rename(columns={ 'average': 'subtask_a', 'text': 'tweet' })\n",
        "\n",
        "training_offensive_A = training_dataset2020_A[training_dataset2020_A['subtask_a'] == 'OFF'].head(50000)\n",
        "training_neutral_A = training_dataset2020_A[training_dataset2020_A['subtask_a'] == 'NOT'].head(50000)\n",
        "\n",
        "training_dataset2020_C['subtask_c'] = training_dataset2020_C.apply(lambda row: convert_averages_c(row), axis=1)\n",
        "training_dataset2020_C = training_dataset2020_C.drop(labels=['id', 'average_ind', 'average_grp', 'average_oth', 'std_ind', 'std_grp', 'std_oth'], axis=1)\n",
        "\n",
        "training_individual_C = training_dataset2020_C[training_dataset2020_C['subtask_c'] == 'IND'].head(63500)\n",
        "training_non_individual_C = training_dataset2020_C[training_dataset2020_C['subtask_c'] != 'IND']\n",
        "\n",
        "training_dataset = training_dataset.append(training_offensive_A)\n",
        "training_dataset = training_dataset.append(training_neutral_A)\n",
        "# training_dataset = training_dataset.append(training_dataset2020_C)\n",
        "training_dataset = training_dataset.append(training_individual_C)\n",
        "training_dataset = training_dataset.append(training_non_individual_C)\n",
        "training_dataset = training_dataset.sample(frac=1, random_state=13)\n",
        "training_dataset\n",
        "'''"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>173034</th>\n",
              "      <td>NaN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@USER @USER That's awful news!ðŸ˜¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54655</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@USER Likewise. Feel free to contact me whenev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30512</th>\n",
              "      <td>NaN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I f*ckd up a whole chipotle burrito in 5 minut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130082</th>\n",
              "      <td>NaN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>its not even been a singluar day and my shenan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42319</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IND</td>\n",
              "      <td>iâ€™m such a terrible texter but i swear ion be ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67528</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IND</td>\n",
              "      <td>Idgaf if itâ€™s 2019 no one better ask me to eat...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51437</th>\n",
              "      <td>NaN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GOD DAMN IT, I MADE THE COVER BUT IT SOUNDS LI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3169</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In a sky full of stars, I think I saw you.   -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63663</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IND</td>\n",
              "      <td>Mcinnes blaming the referee fuck me your squad...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8114</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GRP</td>\n",
              "      <td>@USER The nigga in jail or some?</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>213151 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                              tweet\n",
              "173034 NaN  ...                    @USER @USER That's awful news!ðŸ˜¢\n",
              "54655  NaN  ...  @USER Likewise. Feel free to contact me whenev...\n",
              "30512  NaN  ...  I f*ckd up a whole chipotle burrito in 5 minut...\n",
              "130082 NaN  ...  its not even been a singluar day and my shenan...\n",
              "42319  NaN  ...                                                NaN\n",
              "...     ..  ...                                                ...\n",
              "67528  NaN  ...                                                NaN\n",
              "51437  NaN  ...  GOD DAMN IT, I MADE THE COVER BUT IT SOUNDS LI...\n",
              "3169   NaN  ...  In a sky full of stars, I think I saw you.   -...\n",
              "63663  NaN  ...                                                NaN\n",
              "8114   NaN  ...                                                NaN\n",
              "\n",
              "[213151 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xDBjuNciVDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7de479bc-eb1d-46c0-fe93-5e656268b16f"
      },
      "source": [
        "'''\n",
        "# Additional datasets\n",
        "set_urls = [\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set1.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set2.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set3.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set4.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set5.csv',\n",
        "]\n",
        "sets = [pd.read_csv(url) for url in set_urls]\n",
        "'''"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Additional datasets\\nset_urls = [\\n            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set1.csv',\\n            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set2.csv',\\n            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set3.csv',\\n            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set4.csv',\\n            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set5.csv',\\n]\\nsets = [pd.read_csv(url) for url in set_urls]\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCDhsVxMLnLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3b65565e-63bb-4241-a5ad-c0bf0e63a007"
      },
      "source": [
        "'''\n",
        "def labels_set1(row):\n",
        "  return 'OFF' if 1 in [row['toxic'], row['severe_toxic'], row['obscene'], row['threat'], row['insult'], row['identity_hate']] else 'NOT'\n",
        "\n",
        "def labels_set2(row):\n",
        "  return 'OFF' if row['class'] != 2 else 'NOT'\n",
        "\n",
        "def labels_set_ordinary(row):\n",
        "  return 'OFF' if row['subtask_a'] == 1 else 'NOT'\n",
        "'''"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef labels_set1(row):\\n  return 'OFF' if 1 in [row['toxic'], row['severe_toxic'], row['obscene'], row['threat'], row['insult'], row['identity_hate']] else 'NOT'\\n\\ndef labels_set2(row):\\n  return 'OFF' if row['class'] != 2 else 'NOT'\\n\\ndef labels_set_ordinary(row):\\n  return 'OFF' if row['subtask_a'] == 1 else 'NOT'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0uTpIMiVih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c581759d-2e22-4903-b9b5-e612af26cbcd"
      },
      "source": [
        "'''\n",
        "# Preprocess additional datasets\n",
        "\n",
        "sets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[0] = sets[0].drop('id', axis=1)\n",
        "\n",
        "sets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\n",
        "sets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\n",
        "sets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "sets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\n",
        "sets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\n",
        "\n",
        "sets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[3] = sets[3].drop('Date', axis=1)\n",
        "\n",
        "sets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\n",
        "\n",
        "sets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\n",
        "\n",
        "sets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\n",
        "\n",
        "for s in sets: # one can change to sets_hate_only\n",
        "  training_dataset = training_dataset.append(s)\n",
        "\n",
        "sets_hate_only\n",
        "'''"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Preprocess additional datasets\\n\\nsets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\\nsets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[0] = sets[0].drop('id', axis=1)\\n\\nsets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\\nsets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\\nsets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\\n\\nsets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\\nsets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\\nsets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\\n\\nsets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[3] = sets[3].drop('Date', axis=1)\\n\\nsets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\\n\\nsets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\\n\\nsets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\\n\\nfor s in sets: # one can change to sets_hate_only\\n  training_dataset = training_dataset.append(s)\\n\\nsets_hate_only\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "seed = 13\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = testing_dataset_examples_A['tweet'][testing_dataset_examples_A['tweet'].notnull()]\n",
        "test_labels_A = (testing_dataset_labels_A['label'][testing_dataset_labels_A['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "test_examples_C = testing_dataset_examples_C['tweet'][testing_dataset_examples_C['tweet'].notnull()]\n",
        "test_labels_C = testing_dataset_labels_C['label'][testing_dataset_labels_C['label'].notnull()].replace(c_mapping)\n",
        "\n",
        "training_x = np.array(training_examples_A)\n",
        "validation_x = np.array(validation_examples_A)\n",
        "training_y = np.array(training_labels_A)\n",
        "validation_y = np.array(validation_labels_A)\n",
        "test_x = np.array(test_examples_A)\n",
        "test_y = np.array(test_labels_A)\n",
        "\n",
        "\n",
        "training_x_c = np.array(training_examples_C)\n",
        "validation_x_c = np.array(validation_examples_C)\n",
        "training_y_c = np.array(training_labels_C)\n",
        "validation_y_c = np.array(validation_labels_C)\n",
        "test_x_c = np.array(test_examples_C)\n",
        "test_y_c = np.array(test_labels_C)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1qJTINNPbN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "b013f3ee-0a22-461c-db34-c6d3464c96f9"
      },
      "source": [
        "training_x = np.array(pd.read_csv('training_examples_A.csv'))\n",
        "validation_x = np.array(pd.read_csv('validation_examples_A.csv'))\n",
        "training_y = np.array(pd.read_csv('training_labels_A.csv'))\n",
        "validation_y = np.array(pd.read_csv('validation_labels_A.csv'))\n",
        "\n",
        "training_x_c = np.array(pd.read_csv('training_examples_C_limited.csv'))\n",
        "validation_x_c = np.array(pd.read_csv('validation_examples_C_limited.csv'))\n",
        "training_y_c = np.array(pd.read_csv('training_labels_C_limited.csv'))\n",
        "validation_y_c = np.array(pd.read_csv('validation_labels_C_limited.csv'))\n",
        "\n",
        "testing_examples_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levela.tsv'\n",
        "testing_labels_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levela.csv'\n",
        "testing_examples_C_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levelc.tsv'\n",
        "testing_labels_C_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levelc.csv'\n",
        "\n",
        "testing_dataset_examples_A = np.array(pd.read_csv(testing_examples_A_url, delimiter='\\t'))\n",
        "testing_dataset_labels_A = np.array(pd.read_csv(testing_labels_A_url, delimiter=','))\n",
        "testing_dataset_examples_C = np.array(pd.read_csv(testing_examples_A_url, delimiter='\\t'))\n",
        "testing_dataset_labels_C = np.array(pd.read_csv(testing_labels_A_url, delimiter=','))\n",
        "\n",
        "training_x"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['@USER Us conservatives hope he does.'],\n",
              "       ['Lowkey want to have breakfast for dinner tonight ðŸ¤”'],\n",
              "       ['@USER So black people need white liberals like you to keep them from turning to violence based on things they perceive to be offensive?'],\n",
              "       ...,\n",
              "       ['Rahm Emanuelâ€™s failure is an ill omen for all Democrats\"  URL #TCOT #MAGA #RedNationRising\"'],\n",
              "       ['@USER you are right and you should say it'],\n",
              "       [\"Is it over? 'cause I'm blowin' out the flame\"]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQgHppQoINx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "4d697e2a-077f-48fc-a8b9-23e4d89f76ca"
      },
      "source": [
        "# save randomized datasets to files\n",
        "'''\n",
        "training_examples_A.to_csv('training_examples_A.csv', index=False, index_label=False)\n",
        "validation_examples_A.to_csv('validation_examples_A.csv', index=False, index_label=False)\n",
        "training_labels_A.to_csv('training_labels_A.csv', index=False, index_label=False)\n",
        "validation_labels_A.to_csv('validation_labels_A.csv', index=False, index_label=False)\n",
        "\n",
        "\n",
        "training_examples_C.to_csv('training_examples_C_limited.csv', index=False, index_label=False)\n",
        "validation_examples_C.to_csv('validation_examples_C_limited.csv', index=False, index_label=False)\n",
        "training_labels_C.to_csv('training_labels_C_limited.csv', index=False, index_label=False)\n",
        "validation_labels_C.to_csv('validation_labels_C_limited.csv', index=False, index_label=False)\n",
        "'''"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffÃ¨ -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "        '''\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        # if flag == True:\n",
        "            # suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            # if len(suggestions) > 0:\n",
        "                # edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "        '''\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "e05181f8-ab81-44a6-9618-2cd8897c634d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 21:35:21--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.210.128, 2607:f8b0:400c:c12::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.210.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248381879 (1.2G) [application/zip]\n",
            "Saving to: â€˜wwm_uncased_L-24_H-1024_A-16.zip.3â€™\n",
            "\n",
            "wwm_uncased_L-24_H- 100%[===================>]   1.16G  35.1MB/s    in 34s     \n",
            "\n",
            "2020-03-04 21:35:55 (34.9 MB/s) - â€˜wwm_uncased_L-24_H-1024_A-16.zip.3â€™ saved [1248381879/1248381879]\n",
            "\n",
            "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
            "replace wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  A\n",
            "\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "            \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting theÂ module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"wwm_uncased_L-24_H-1024_A-16\"\n",
        "\n",
        "config_path = \"/content/gdrive/My Drive/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/gdrive/My Drive/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/gdrive/My Drive/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        splitted = re.split(r'[.?!]\\s*', text_a)\n",
        "        text_b = splitted[-1]\n",
        "        text_a.replace(text_b, '')\n",
        "\n",
        "\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT KerasÂ layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", \"lstm\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "\n",
        "            if self.pooling == \"lstm\":\n",
        "              lstm = tf.keras.layers.LSTM(1)\n",
        "              pooled = lstm(result)\n",
        "\n",
        "            else:\n",
        "              input_mask = tf.cast(input_mask, tf.float32)\n",
        "              mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "              masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                      tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "              \n",
        "              if self.pooling == \"mean\":\n",
        "                pooled = masked_reduce_mean(result, input_mask)\n",
        "              else:\n",
        "                pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "67cb6834-4081-41c5-e6e4-e9a0f3477e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=200, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=24, verbose=False)\n",
        "\n",
        "l1 = tf.keras.layers.Dense(768, activation='relu')\n",
        "l2 = tf.keras.layers.Dense(768, activation='relu')\n",
        "# l3 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "d = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(d(l2(l1(encoder(inp)))))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DmN-x8-FgAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall + tf.keras.backend.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-6kHUmZ7Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)\n",
        "\n",
        "# print(class_weights) # [majority class weight, minority class weight]\n",
        "\n",
        "minority_count = len(training_labels_A[training_labels_A == 1])\n",
        "majority_count = len(training_labels_A[training_labels_A == 0])\n",
        "\n",
        "def weighted_loss(actual, predicted):\n",
        "    weights = class_weights\n",
        "    # weights = [0.0714, 1.0]\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "1edf90c1-e3a6-4c26-837b-7cbd6654fca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6, ),\n",
        "      loss=weighted_loss,\n",
        "      metrics=[f1_m])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer_2 (BertLayer)     (None, 1024)              335141888 \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 768)               787200    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 336,520,449\n",
            "Trainable params: 304,737,537\n",
            "Non-trainable params: 31,782,912\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2cppA_UDg5W",
        "outputId": "3e43cfe6-2c51-47a4-f347-a829c2a0efd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_large_H8_S256_B32.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=16, epochs=1, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "outputId": "c097470d-a39e-428f-8434-5752767bdcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nefrYRa4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mM9p7JRErm4",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict(test_x, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkP3H-PCEwxP",
        "colab_type": "text"
      },
      "source": [
        "### Count f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDV8k9M4kBB",
        "colab_type": "code",
        "outputId": "c7695952-d8c5-4f56-b2f1-10082e98efa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def round_int(array, threshold):\n",
        "  return np.array([np.ceil(x) if x >= threshold else np.floor(x) for x in array]).astype(np.float64)\n",
        "\n",
        "flat_predicted = round_int(predicted[:, 0], 0.4)\n",
        "print(f1_score(test_y, flat_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7208791208791209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2-MpWJNtQw",
        "colab_type": "code",
        "outputId": "071d8f01-5014-40cc-fdcd-59f8ade008b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "testing_dataset_examples_A = pd.read_csv('Colab Notebooks/test_a_tweets.tsv', delimiter='\\t')\n",
        "test_examples_A = np.array(testing_dataset_examples_A['tweet'][testing_dataset_examples_A['tweet'].notnull()])\n",
        "test_examples_A"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Â¿Who the fuck is Yoru?',\n",
              "       \"@USER @USER He's an evil law breaker that should be in prison with his criminal heartless family.\",\n",
              "       'Now hiring for 49 #Labor job opportunities in #Minnesota. Click the link in our bio to see them.',\n",
              "       ...,\n",
              "       '@USER @USER @USER @USER @USER Q for CSIA would both be functional OR only NMIA ðŸ¤”',\n",
              "       '@USER Her parents should be in jail for child-abuse  Jesus Christ, poor kid brainwashed',\n",
              "       '@USER @USER @USER @USER @USER Trump is a trash criminal president.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLSUY6np71w6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_emb = tf.keras.models.Model(inputs=[model.layers[0].input], outputs=[model.layers[-3].output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouNObp2ZLTJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = model_emb.predict(test_examples_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqTJNKbPLUkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez('2020-TEST-bert-embeddings.npz', data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"bert_H4_S256_B64.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "outputId": "4f7cd490-6ea6-47b0-b8ad-26779aa5a5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"bert_H4_S256_B64.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_H4_S256_B64.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-69d6a0b5ff92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = tf.keras.models.model_from_json(json.load(open(\"bert_H4_S256_B64.json\")), \n\u001b[0m\u001b[1;32m      2\u001b[0m                                         custom_objects={\"BertLayer\": BertLayer})\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_H4_S256_B64.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert_H4_S256_B64.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAiiJEnfYpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}